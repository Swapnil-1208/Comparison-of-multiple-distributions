{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPannVkv6/9NEnVZd4W0LRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swapnil-1208/Comparison-of-multiple-distributions/blob/main/Deans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdvx-KswNnR0",
        "outputId": "9c36ef63-3f36-466b-88c0-b8b0b6672d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DEAN'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 8), reused 8 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/psorus/DEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ialF0bSB5wMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mJ2f0yUJ6hsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7kZTNN_QA8M",
        "outputId": "1affe679-e2f1-43f7-a78f-017073fc5671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3irSdTmN2r3",
        "outputId": "1a404f25-a044-441d-80a2-795590ebc020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "  inflating: /content/data/outlier_data.csv  \n",
            "  inflating: /content/data/train_data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mRXzPLXQHc6",
        "outputId": "ccabc64a-cf39-4ab2-a1f0-f1f325ac63f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "  inflating: /content/data/outlier_data.csv  \n",
            "  inflating: /content/data/train_data.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/DEAN/main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPGAbjwSAvB",
        "outputId": "73544fa7-5934-4ba6-80ba-a2b52f7b53ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " tf.math.squared_difference_41   (None, 7)           0           ['tf.convert_to_tensor_41[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_41[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_82 (TFOpLa  (None,)             0           ['tf.math.squared_difference_41[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_83 (TFOpLa  ()                  0           ['tf.math.reduce_mean_82[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_41 (AddLoss)          ()                   0           ['tf.math.reduce_mean_83[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/41/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0019 - val_loss: 1.2309e-08\n",
            "Epoch 2/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 2.4147e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4090e-05 - val_loss: 3.2368e-08\n",
            "Epoch 3/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 2.3234e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3280e-05 - val_loss: 4.6493e-05\n",
            "Epoch 4/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.8648e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8640e-05 - val_loss: 3.8889e-05\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2762e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2760e-05 - val_loss: 4.7114e-06\n",
            "Epoch 6/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.2664e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/41/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2578e-05 - val_loss: 8.7466e-09\n",
            "Epoch 7/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.3021e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2973e-05 - val_loss: 1.3327e-08\n",
            "Epoch 8/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.2024e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2031e-05 - val_loss: 5.9016e-06\n",
            "Epoch 9/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.3024e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2964e-05 - val_loss: 4.0725e-07\n",
            "Epoch 10/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.2490e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2479e-05 - val_loss: 1.4827e-07\n",
            "Epoch 11/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2424e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3112e-05 - val_loss: 4.6060e-04\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5878974056609789\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_43 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 7)            49          ['input_43[0][0]']               \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 7)            49          ['dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_42 (TFOpLambda)   (None, 7)            0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_42 (TFOpLambd  (None, 7)           0           ['tf.ones_like_42[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_42 (TFOpL  (None, 7)           0           ['tf.math.multiply_42[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_42 (TFOpLambda)        (None, 7)            0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_42   (None, 7)           0           ['tf.convert_to_tensor_42[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_42[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_84 (TFOpLa  (None,)             0           ['tf.math.squared_difference_42[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_85 (TFOpLa  ()                  0           ['tf.math.reduce_mean_84[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_42 (AddLoss)          ()                   0           ['tf.math.reduce_mean_85[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/42/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0021 - val_loss: 2.4375e-06\n",
            "Epoch 2/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.0526e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0506e-05 - val_loss: 3.2511e-05\n",
            "Epoch 3/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 3.0191e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.9973e-05 - val_loss: 2.6097e-05\n",
            "Epoch 4/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.0399e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0352e-05 - val_loss: 3.7921e-06\n",
            "Epoch 5/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 2.9098e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/42/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.8966e-05 - val_loss: 1.0154e-06\n",
            "Epoch 6/500\n",
            "4197/4236 [============================>.] - ETA: 0s - loss: 2.7087e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6859e-05 - val_loss: 3.1206e-06\n",
            "Epoch 7/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.6963e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6936e-05 - val_loss: 4.1761e-06\n",
            "Epoch 8/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.5326e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5287e-05 - val_loss: 6.9106e-06\n",
            "Epoch 9/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 1.5454e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5421e-05 - val_loss: 4.3576e-06\n",
            "Epoch 10/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.5171e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5204e-05 - val_loss: 2.3182e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 20s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8272929095538645\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_44 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 7)            49          ['input_44[0][0]']               \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 7)            49          ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_43 (TFOpLambda)   (None, 7)            0           ['dense_87[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_43 (TFOpLambd  (None, 7)           0           ['tf.ones_like_43[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_43 (TFOpL  (None, 7)           0           ['tf.math.multiply_43[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_43 (TFOpLambda)        (None, 7)            0           ['dense_87[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_43   (None, 7)           0           ['tf.convert_to_tensor_43[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_43[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_86 (TFOpLa  (None,)             0           ['tf.math.squared_difference_43[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_87 (TFOpLa  ()                  0           ['tf.math.reduce_mean_86[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_43 (AddLoss)          ()                   0           ['tf.math.reduce_mean_87[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/43/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0018 - val_loss: 7.7041e-06\n",
            "Epoch 2/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 3.0935e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00000, saving model to results/43/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0869e-05 - val_loss: 2.0730e-09\n",
            "Epoch 3/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 2.7200e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7193e-05 - val_loss: 2.6959e-05\n",
            "Epoch 4/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 2.2941e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2908e-05 - val_loss: 7.3355e-06\n",
            "Epoch 5/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.6379e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6363e-05 - val_loss: 3.8541e-09\n",
            "Epoch 6/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.2612e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2607e-05 - val_loss: 8.8094e-07\n",
            "Epoch 7/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.1827e-05\n",
            "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to results/43/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1801e-05 - val_loss: 8.8910e-10\n",
            "Epoch 8/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 1.1902e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1818e-05 - val_loss: 1.7590e-07\n",
            "Epoch 9/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2413e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2478e-05 - val_loss: 2.3468e-07\n",
            "Epoch 10/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.2270e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2221e-05 - val_loss: 2.4285e-09\n",
            "Epoch 11/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.1835e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1912e-05 - val_loss: 1.1079e-05\n",
            "Epoch 12/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.2180e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2180e-05 - val_loss: 3.9923e-05\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.7770709535625412\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_45 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 7)            49          ['input_45[0][0]']               \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 7)            49          ['dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_44 (TFOpLambda)   (None, 7)            0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_44 (TFOpLambd  (None, 7)           0           ['tf.ones_like_44[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_44 (TFOpL  (None, 7)           0           ['tf.math.multiply_44[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_44 (TFOpLambda)        (None, 7)            0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_44   (None, 7)           0           ['tf.convert_to_tensor_44[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_44[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_88 (TFOpLa  (None,)             0           ['tf.math.squared_difference_44[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_89 (TFOpLa  ()                  0           ['tf.math.reduce_mean_88[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_44 (AddLoss)          ()                   0           ['tf.math.reduce_mean_89[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/44/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0020 - val_loss: 2.8249e-09\n",
            "Epoch 2/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 3.2341e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.2333e-05 - val_loss: 1.1450e-05\n",
            "Epoch 3/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 2.3796e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3751e-05 - val_loss: 1.1542e-04\n",
            "Epoch 4/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 2.1115e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.1081e-05 - val_loss: 1.8147e-06\n",
            "Epoch 5/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.9870e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9791e-05 - val_loss: 1.3308e-05\n",
            "Epoch 6/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 2.0310e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0234e-05 - val_loss: 3.2060e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5605868653270124\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_46 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 7)            49          ['input_46[0][0]']               \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 7)            49          ['dense_90[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_45 (TFOpLambda)   (None, 7)            0           ['dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_45 (TFOpLambd  (None, 7)           0           ['tf.ones_like_45[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_45 (TFOpL  (None, 7)           0           ['tf.math.multiply_45[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_45 (TFOpLambda)        (None, 7)            0           ['dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_45   (None, 7)           0           ['tf.convert_to_tensor_45[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_45[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_90 (TFOpLa  (None,)             0           ['tf.math.squared_difference_45[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_91 (TFOpLa  ()                  0           ['tf.math.reduce_mean_90[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_45 (AddLoss)          ()                   0           ['tf.math.reduce_mean_91[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/45/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0018 - val_loss: 6.9535e-06\n",
            "Epoch 2/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 3.9873e-05\n",
            "Epoch 2: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.9953e-05 - val_loss: 3.5649e-05\n",
            "Epoch 3/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 3.1071e-05\n",
            "Epoch 3: val_loss improved from 0.00001 to 0.00000, saving model to results/45/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0939e-05 - val_loss: 2.2241e-07\n",
            "Epoch 4/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 3.1991e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.1889e-05 - val_loss: 4.1253e-07\n",
            "Epoch 5/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 2.7724e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/45/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7633e-05 - val_loss: 1.4166e-07\n",
            "Epoch 6/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3988e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4028e-05 - val_loss: 2.6523e-05\n",
            "Epoch 7/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.0990e-05\n",
            "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to results/45/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0951e-05 - val_loss: 1.2962e-09\n",
            "Epoch 8/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 9.4006e-06\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 9.4006e-06 - val_loss: 3.0025e-07\n",
            "Epoch 9/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2964e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2909e-05 - val_loss: 3.6488e-09\n",
            "Epoch 10/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 8.8931e-06\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 9.2145e-06 - val_loss: 1.9753e-04\n",
            "Epoch 11/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 9.9383e-06\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 9.9074e-06 - val_loss: 3.2363e-07\n",
            "Epoch 12/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.0401e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0387e-05 - val_loss: 3.8769e-06\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5527519982025276\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_47 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 7)            49          ['input_47[0][0]']               \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 7)            49          ['dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_46 (TFOpLambda)   (None, 7)            0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_46 (TFOpLambd  (None, 7)           0           ['tf.ones_like_46[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_46 (TFOpL  (None, 7)           0           ['tf.math.multiply_46[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_46 (TFOpLambda)        (None, 7)            0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_46   (None, 7)           0           ['tf.convert_to_tensor_46[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_46[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_92 (TFOpLa  (None,)             0           ['tf.math.squared_difference_46[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_93 (TFOpLa  ()                  0           ['tf.math.reduce_mean_92[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_46 (AddLoss)          ()                   0           ['tf.math.reduce_mean_93[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/46/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0021 - val_loss: 9.4766e-09\n",
            "Epoch 2/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 2.1781e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2421e-05 - val_loss: 1.5387e-04\n",
            "Epoch 3/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 2.1795e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.1694e-05 - val_loss: 2.9970e-07\n",
            "Epoch 4/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.7130e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7127e-05 - val_loss: 5.7045e-06\n",
            "Epoch 5/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.3090e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3058e-05 - val_loss: 1.9352e-06\n",
            "Epoch 6/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.2061e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2041e-05 - val_loss: 1.9369e-08\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5931296788299947\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_48 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 7)            49          ['input_48[0][0]']               \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 7)            49          ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_47 (TFOpLambda)   (None, 7)            0           ['dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_47 (TFOpLambd  (None, 7)           0           ['tf.ones_like_47[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_47 (TFOpL  (None, 7)           0           ['tf.math.multiply_47[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_47 (TFOpLambda)        (None, 7)            0           ['dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_47   (None, 7)           0           ['tf.convert_to_tensor_47[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_47[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_94 (TFOpLa  (None,)             0           ['tf.math.squared_difference_47[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_95 (TFOpLa  ()                  0           ['tf.math.reduce_mean_94[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_47 (AddLoss)          ()                   0           ['tf.math.reduce_mean_95[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/47/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0020 - val_loss: 1.5542e-09\n",
            "Epoch 2/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 2.6811e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6660e-05 - val_loss: 6.5090e-04\n",
            "Epoch 3/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 2.0382e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.0339e-05 - val_loss: 2.2605e-05\n",
            "Epoch 4/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.3978e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3982e-05 - val_loss: 4.2952e-04\n",
            "Epoch 5/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.0824e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0809e-05 - val_loss: 1.3659e-06\n",
            "Epoch 6/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.0175e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0169e-05 - val_loss: 5.8659e-06\n",
            "17649/17649 [==============================] - 20s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5939971471233625\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_49 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 7)            49          ['input_49[0][0]']               \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 7)            49          ['dense_96[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_48 (TFOpLambda)   (None, 7)            0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_48 (TFOpLambd  (None, 7)           0           ['tf.ones_like_48[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_48 (TFOpL  (None, 7)           0           ['tf.math.multiply_48[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_48 (TFOpLambda)        (None, 7)            0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_48   (None, 7)           0           ['tf.convert_to_tensor_48[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_48[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_96 (TFOpLa  (None,)             0           ['tf.math.squared_difference_48[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_97 (TFOpLa  ()                  0           ['tf.math.reduce_mean_96[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_48 (AddLoss)          ()                   0           ['tf.math.reduce_mean_97[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0019 - val_loss: 3.1343e-05\n",
            "Epoch 2/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 3.4355e-05\n",
            "Epoch 2: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 11s 3ms/step - loss: 3.5479e-05 - val_loss: 1.6648e-04\n",
            "Epoch 3/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.8317e-05\n",
            "Epoch 3: val_loss improved from 0.00003 to 0.00002, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.8343e-05 - val_loss: 2.0579e-05\n",
            "Epoch 4/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.9428e-05\n",
            "Epoch 4: val_loss improved from 0.00002 to 0.00002, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9428e-05 - val_loss: 1.6156e-05\n",
            "Epoch 5/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.5098e-05\n",
            "Epoch 5: val_loss improved from 0.00002 to 0.00000, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5091e-05 - val_loss: 8.1959e-07\n",
            "Epoch 6/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.1912e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1892e-05 - val_loss: 1.7624e-08\n",
            "Epoch 7/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2636e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2565e-05 - val_loss: 2.0167e-08\n",
            "Epoch 8/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.2163e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2128e-05 - val_loss: 2.1035e-06\n",
            "Epoch 9/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2832e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/48/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2829e-05 - val_loss: 3.8811e-09\n",
            "Epoch 10/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.2089e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2061e-05 - val_loss: 2.0451e-06\n",
            "Epoch 11/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 1.2025e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1931e-05 - val_loss: 1.0861e-07\n",
            "Epoch 12/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.1785e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1763e-05 - val_loss: 1.2506e-08\n",
            "Epoch 13/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.2229e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2198e-05 - val_loss: 8.8069e-08\n",
            "Epoch 14/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 1.1767e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1710e-05 - val_loss: 1.6112e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5130418320696091\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_50 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 7)            49          ['input_50[0][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 7)            49          ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " tf.ones_like_49 (TFOpLambda)   (None, 7)            0           ['dense_99[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.multiply_49 (TFOpLambd  (None, 7)           0           ['tf.ones_like_49[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_49 (TFOpL  (None, 7)           0           ['tf.math.multiply_49[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_49 (TFOpLambda)        (None, 7)            0           ['dense_99[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_49   (None, 7)           0           ['tf.convert_to_tensor_49[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_49[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_98 (TFOpLa  (None,)             0           ['tf.math.squared_difference_49[0\n",
            " mbda)                                                           ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_99 (TFOpLa  ()                  0           ['tf.math.reduce_mean_98[0][0]'] \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " add_loss_49 (AddLoss)          ()                   0           ['tf.math.reduce_mean_99[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/49/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0020 - val_loss: 1.0774e-09\n",
            "Epoch 2/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 3.0352e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.0360e-05 - val_loss: 1.8059e-05\n",
            "Epoch 3/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 3.0524e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0322e-05 - val_loss: 2.0279e-08\n",
            "Epoch 4/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 3.0080e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.0013e-05 - val_loss: 7.1050e-08\n",
            "Epoch 5/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.2702e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2603e-05 - val_loss: 1.3306e-05\n",
            "Epoch 6/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.6245e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6181e-05 - val_loss: 2.4827e-07\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5384160745197712\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_51 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 7)            49          ['input_51[0][0]']               \n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 7)            49          ['dense_100[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_50 (TFOpLambda)   (None, 7)            0           ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_50 (TFOpLambd  (None, 7)           0           ['tf.ones_like_50[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_50 (TFOpL  (None, 7)           0           ['tf.math.multiply_50[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_50 (TFOpLambda)        (None, 7)            0           ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_50   (None, 7)           0           ['tf.convert_to_tensor_50[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_50[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_100 (TFOpL  (None,)             0           ['tf.math.squared_difference_50[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_101 (TFOpL  ()                  0           ['tf.math.reduce_mean_100[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_50 (AddLoss)          ()                   0           ['tf.math.reduce_mean_101[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 0.0033\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/50/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0033 - val_loss: 9.9288e-10\n",
            "Epoch 2/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 7.0027e-06\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 6.9997e-06 - val_loss: 2.0297e-06\n",
            "Epoch 3/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.0242e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0221e-05 - val_loss: 6.2069e-08\n",
            "Epoch 4/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.0431e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0414e-05 - val_loss: 1.2831e-06\n",
            "Epoch 5/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.0010e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0175e-05 - val_loss: 3.9938e-06\n",
            "Epoch 6/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.0119e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0114e-05 - val_loss: 1.7185e-08\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5565360695288046\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_52 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 7)            49          ['input_52[0][0]']               \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 7)            49          ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_51 (TFOpLambda)   (None, 7)            0           ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_51 (TFOpLambd  (None, 7)           0           ['tf.ones_like_51[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_51 (TFOpL  (None, 7)           0           ['tf.math.multiply_51[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_51 (TFOpLambda)        (None, 7)            0           ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_51   (None, 7)           0           ['tf.convert_to_tensor_51[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_51[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_102 (TFOpL  (None,)             0           ['tf.math.squared_difference_51[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_103 (TFOpL  ()                  0           ['tf.math.reduce_mean_102[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_51 (AddLoss)          ()                   0           ['tf.math.reduce_mean_103[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00008, saving model to results/51/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0017 - val_loss: 7.9030e-05\n",
            "Epoch 2/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 4.7035e-05\n",
            "Epoch 2: val_loss improved from 0.00008 to 0.00000, saving model to results/51/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.7637e-05 - val_loss: 8.4850e-07\n",
            "Epoch 3/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 4.1377e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.1523e-05 - val_loss: 4.6688e-04\n",
            "Epoch 4/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 3.0269e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0171e-05 - val_loss: 4.6083e-05\n",
            "Epoch 5/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 2.5155e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/51/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5037e-05 - val_loss: 6.0212e-08\n",
            "Epoch 6/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.2854e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.2849e-05 - val_loss: 3.9424e-06\n",
            "Epoch 7/500\n",
            "4200/4236 [============================>.] - ETA: 0s - loss: 1.8555e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8401e-05 - val_loss: 5.1259e-07\n",
            "Epoch 8/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.3578e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3578e-05 - val_loss: 1.6910e-07\n",
            "Epoch 9/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.2508e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2446e-05 - val_loss: 2.3515e-07\n",
            "Epoch 10/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.1837e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2153e-05 - val_loss: 9.9033e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8374382460119671\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_53 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 7)            49          ['input_53[0][0]']               \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 7)            49          ['dense_104[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_52 (TFOpLambda)   (None, 7)            0           ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_52 (TFOpLambd  (None, 7)           0           ['tf.ones_like_52[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_52 (TFOpL  (None, 7)           0           ['tf.math.multiply_52[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_52 (TFOpLambda)        (None, 7)            0           ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_52   (None, 7)           0           ['tf.convert_to_tensor_52[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_52[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_104 (TFOpL  (None,)             0           ['tf.math.squared_difference_52[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_105 (TFOpL  ()                  0           ['tf.math.reduce_mean_104[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_52 (AddLoss)          ()                   0           ['tf.math.reduce_mean_105[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/52/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0018 - val_loss: 1.0533e-05\n",
            "Epoch 2/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 4.5911e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00000, saving model to results/52/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.5761e-05 - val_loss: 7.7152e-08\n",
            "Epoch 3/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.9935e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.9871e-05 - val_loss: 2.5259e-06\n",
            "Epoch 4/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 3.4856e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4729e-05 - val_loss: 3.1130e-06\n",
            "Epoch 5/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 3.5598e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.5598e-05 - val_loss: 1.7012e-05\n",
            "Epoch 6/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 2.9487e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.9425e-05 - val_loss: 1.5885e-07\n",
            "Epoch 7/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.4474e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4401e-05 - val_loss: 6.4960e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5866819997243784\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_54 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 7)            49          ['input_54[0][0]']               \n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 7)            49          ['dense_106[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_53 (TFOpLambda)   (None, 7)            0           ['dense_107[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_53 (TFOpLambd  (None, 7)           0           ['tf.ones_like_53[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_53 (TFOpL  (None, 7)           0           ['tf.math.multiply_53[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_53 (TFOpLambda)        (None, 7)            0           ['dense_107[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_53   (None, 7)           0           ['tf.convert_to_tensor_53[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_53[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_106 (TFOpL  (None,)             0           ['tf.math.squared_difference_53[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_107 (TFOpL  ()                  0           ['tf.math.reduce_mean_106[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_53 (AddLoss)          ()                   0           ['tf.math.reduce_mean_107[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to results/53/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0017 - val_loss: 3.8883e-05\n",
            "Epoch 2/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 4.7829e-05\n",
            "Epoch 2: val_loss improved from 0.00004 to 0.00000, saving model to results/53/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.7469e-05 - val_loss: 3.7666e-06\n",
            "Epoch 3/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 3.7216e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.7238e-05 - val_loss: 1.1028e-04\n",
            "Epoch 4/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 3.2656e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.2616e-05 - val_loss: 1.4199e-05\n",
            "Epoch 5/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.1805e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.1821e-05 - val_loss: 3.8910e-05\n",
            "Epoch 6/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 2.8646e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.8593e-05 - val_loss: 5.3303e-05\n",
            "Epoch 7/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.9744e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9724e-05 - val_loss: 5.6203e-05\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8626299255983326\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_55 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 7)            49          ['input_55[0][0]']               \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 7)            49          ['dense_108[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_54 (TFOpLambda)   (None, 7)            0           ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_54 (TFOpLambd  (None, 7)           0           ['tf.ones_like_54[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_54 (TFOpL  (None, 7)           0           ['tf.math.multiply_54[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_54 (TFOpLambda)        (None, 7)            0           ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_54   (None, 7)           0           ['tf.convert_to_tensor_54[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_54[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_108 (TFOpL  (None,)             0           ['tf.math.squared_difference_54[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_109 (TFOpL  ()                  0           ['tf.math.reduce_mean_108[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_54 (AddLoss)          ()                   0           ['tf.math.reduce_mean_109[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00002, saving model to results/54/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0019 - val_loss: 2.2025e-05\n",
            "Epoch 2/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 3.8971e-05\n",
            "Epoch 2: val_loss improved from 0.00002 to 0.00000, saving model to results/54/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.8943e-05 - val_loss: 8.2163e-09\n",
            "Epoch 3/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 3.2112e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.1893e-05 - val_loss: 1.0248e-05\n",
            "Epoch 4/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 2.6053e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6050e-05 - val_loss: 3.7761e-05\n",
            "Epoch 5/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.2883e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2883e-05 - val_loss: 2.4854e-08\n",
            "Epoch 6/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.8476e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8424e-05 - val_loss: 1.4613e-05\n",
            "Epoch 7/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.2315e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2303e-05 - val_loss: 3.1587e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5315121075899734\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_56 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 7)            49          ['input_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 7)            49          ['dense_110[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_55 (TFOpLambda)   (None, 7)            0           ['dense_111[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_55 (TFOpLambd  (None, 7)           0           ['tf.ones_like_55[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_55 (TFOpL  (None, 7)           0           ['tf.math.multiply_55[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_55 (TFOpLambda)        (None, 7)            0           ['dense_111[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_55   (None, 7)           0           ['tf.convert_to_tensor_55[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_55[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_110 (TFOpL  (None,)             0           ['tf.math.squared_difference_55[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_111 (TFOpL  ()                  0           ['tf.math.reduce_mean_110[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_55 (AddLoss)          ()                   0           ['tf.math.reduce_mean_111[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 0.0016\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/55/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0016 - val_loss: 3.7422e-09\n",
            "Epoch 2/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 4.2628e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.2291e-05 - val_loss: 9.7664e-06\n",
            "Epoch 3/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.4398e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4398e-05 - val_loss: 2.0289e-06\n",
            "Epoch 4/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 2.4919e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4831e-05 - val_loss: 1.6937e-08\n",
            "Epoch 5/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 2.3906e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3883e-05 - val_loss: 1.2967e-08\n",
            "Epoch 6/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.2050e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.2048e-05 - val_loss: 2.7374e-05\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.7131736434249114\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_57 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 7)            49          ['input_57[0][0]']               \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 7)            49          ['dense_112[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_56 (TFOpLambda)   (None, 7)            0           ['dense_113[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_56 (TFOpLambd  (None, 7)           0           ['tf.ones_like_56[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_56 (TFOpL  (None, 7)           0           ['tf.math.multiply_56[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_56 (TFOpLambda)        (None, 7)            0           ['dense_113[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_56   (None, 7)           0           ['tf.convert_to_tensor_56[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_56[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_112 (TFOpL  (None,)             0           ['tf.math.squared_difference_56[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_113 (TFOpL  ()                  0           ['tf.math.reduce_mean_112[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_56 (AddLoss)          ()                   0           ['tf.math.reduce_mean_113[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/56/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0021 - val_loss: 2.4943e-07\n",
            "Epoch 2/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 2.5719e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.5640e-05 - val_loss: 2.7977e-07\n",
            "Epoch 3/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 2.2743e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2732e-05 - val_loss: 4.8037e-05\n",
            "Epoch 4/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 2.4314e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4228e-05 - val_loss: 3.5087e-07\n",
            "Epoch 5/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.2797e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2762e-05 - val_loss: 3.2884e-06\n",
            "Epoch 6/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.4432e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4432e-05 - val_loss: 2.9575e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.473200551649075\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_58 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_114 (Dense)              (None, 7)            49          ['input_58[0][0]']               \n",
            "                                                                                                  \n",
            " dense_115 (Dense)              (None, 7)            49          ['dense_114[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_57 (TFOpLambda)   (None, 7)            0           ['dense_115[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_57 (TFOpLambd  (None, 7)           0           ['tf.ones_like_57[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_57 (TFOpL  (None, 7)           0           ['tf.math.multiply_57[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_57 (TFOpLambda)        (None, 7)            0           ['dense_115[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_57   (None, 7)           0           ['tf.convert_to_tensor_57[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_57[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_114 (TFOpL  (None,)             0           ['tf.math.squared_difference_57[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_115 (TFOpL  ()                  0           ['tf.math.reduce_mean_114[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_57 (AddLoss)          ()                   0           ['tf.math.reduce_mean_115[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 0.0022\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/57/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0022 - val_loss: 9.2828e-12\n",
            "Epoch 2/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 2.2911e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2879e-05 - val_loss: 4.1840e-05\n",
            "Epoch 3/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.6225e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6221e-05 - val_loss: 1.5946e-10\n",
            "Epoch 4/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 1.4841e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4746e-05 - val_loss: 2.4440e-10\n",
            "Epoch 5/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.3604e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3560e-05 - val_loss: 3.0778e-06\n",
            "Epoch 6/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.5275e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5272e-05 - val_loss: 4.1873e-09\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "17649/17649 [==============================] - 27s 2ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5387454987820685\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_59 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_116 (Dense)              (None, 7)            49          ['input_59[0][0]']               \n",
            "                                                                                                  \n",
            " dense_117 (Dense)              (None, 7)            49          ['dense_116[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_58 (TFOpLambda)   (None, 7)            0           ['dense_117[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_58 (TFOpLambd  (None, 7)           0           ['tf.ones_like_58[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_58 (TFOpL  (None, 7)           0           ['tf.math.multiply_58[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_58 (TFOpLambda)        (None, 7)            0           ['dense_117[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_58   (None, 7)           0           ['tf.convert_to_tensor_58[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_58[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_116 (TFOpL  (None,)             0           ['tf.math.squared_difference_58[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_117 (TFOpL  ()                  0           ['tf.math.reduce_mean_116[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_58 (AddLoss)          ()                   0           ['tf.math.reduce_mean_117[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00026, saving model to results/58/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0020 - val_loss: 2.5636e-04\n",
            "Epoch 2/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 2.7572e-05\n",
            "Epoch 2: val_loss improved from 0.00026 to 0.00000, saving model to results/58/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7526e-05 - val_loss: 2.2343e-07\n",
            "Epoch 3/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 2.5575e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5478e-05 - val_loss: 7.2705e-04\n",
            "Epoch 4/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.7630e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.7518e-05 - val_loss: 2.1280e-05\n",
            "Epoch 5/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.3995e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3955e-05 - val_loss: 1.5514e-05\n",
            "Epoch 6/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.5288e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/58/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5202e-05 - val_loss: 3.5008e-08\n",
            "Epoch 7/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.3454e-05\n",
            "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to results/58/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3423e-05 - val_loss: 1.1044e-12\n",
            "Epoch 8/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.3583e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3519e-05 - val_loss: 1.6875e-11\n",
            "Epoch 9/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 1.4341e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4265e-05 - val_loss: 2.5475e-08\n",
            "Epoch 10/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 1.3872e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3784e-05 - val_loss: 1.6757e-09\n",
            "Epoch 11/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.4147e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4100e-05 - val_loss: 2.0625e-11\n",
            "Epoch 12/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.3305e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3255e-05 - val_loss: 7.9710e-10\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999037188629603\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_60 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_118 (Dense)              (None, 7)            49          ['input_60[0][0]']               \n",
            "                                                                                                  \n",
            " dense_119 (Dense)              (None, 7)            49          ['dense_118[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_59 (TFOpLambda)   (None, 7)            0           ['dense_119[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_59 (TFOpLambd  (None, 7)           0           ['tf.ones_like_59[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_59 (TFOpL  (None, 7)           0           ['tf.math.multiply_59[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_59 (TFOpLambda)        (None, 7)            0           ['dense_119[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_59   (None, 7)           0           ['tf.convert_to_tensor_59[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_59[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_118 (TFOpL  (None,)             0           ['tf.math.squared_difference_59[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_119 (TFOpL  ()                  0           ['tf.math.reduce_mean_118[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_59 (AddLoss)          ()                   0           ['tf.math.reduce_mean_119[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 0.0025\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/59/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0025 - val_loss: 3.7041e-09\n",
            "Epoch 2/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.2372e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2952e-05 - val_loss: 1.4063e-05\n",
            "Epoch 3/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 1.4959e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4998e-05 - val_loss: 1.8928e-04\n",
            "Epoch 4/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.6485e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6485e-05 - val_loss: 4.9165e-08\n",
            "Epoch 5/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 1.6041e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6413e-05 - val_loss: 3.5800e-06\n",
            "Epoch 6/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.4592e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4531e-05 - val_loss: 1.9364e-06\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5777197282400073\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_120 (Dense)              (None, 7)            49          ['input_61[0][0]']               \n",
            "                                                                                                  \n",
            " dense_121 (Dense)              (None, 7)            49          ['dense_120[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_60 (TFOpLambda)   (None, 7)            0           ['dense_121[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_60 (TFOpLambd  (None, 7)           0           ['tf.ones_like_60[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_60 (TFOpL  (None, 7)           0           ['tf.math.multiply_60[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_60 (TFOpLambda)        (None, 7)            0           ['dense_121[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_60   (None, 7)           0           ['tf.convert_to_tensor_60[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_60[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_120 (TFOpL  (None,)             0           ['tf.math.squared_difference_60[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_121 (TFOpL  ()                  0           ['tf.math.reduce_mean_120[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_60 (AddLoss)          ()                   0           ['tf.math.reduce_mean_121[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00006, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0024 - val_loss: 6.4719e-05\n",
            "Epoch 2/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 4.2589e-05\n",
            "Epoch 2: val_loss improved from 0.00006 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.2509e-05 - val_loss: 4.5465e-06\n",
            "Epoch 3/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.4186e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4096e-05 - val_loss: 2.4044e-06\n",
            "Epoch 4/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 2.0755e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.0778e-05 - val_loss: 4.6512e-06\n",
            "Epoch 5/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.8613e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8613e-05 - val_loss: 1.0972e-06\n",
            "Epoch 6/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.5737e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5752e-05 - val_loss: 8.0950e-06\n",
            "Epoch 7/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.3243e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3165e-05 - val_loss: 1.3409e-06\n",
            "Epoch 8/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 1.3365e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3348e-05 - val_loss: 5.8532e-07\n",
            "Epoch 9/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.2195e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2133e-05 - val_loss: 2.7721e-07\n",
            "Epoch 10/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.2536e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2652e-05 - val_loss: 5.3887e-06\n",
            "Epoch 11/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2258e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3007e-05 - val_loss: 1.4146e-04\n",
            "Epoch 12/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.2282e-05\n",
            "Epoch 12: val_loss improved from 0.00000 to 0.00000, saving model to results/60/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2244e-05 - val_loss: 6.6563e-09\n",
            "Epoch 13/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.2009e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1978e-05 - val_loss: 1.6828e-07\n",
            "Epoch 14/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.2337e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2277e-05 - val_loss: 4.3605e-08\n",
            "Epoch 15/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.2108e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2235e-05 - val_loss: 1.0560e-05\n",
            "Epoch 16/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 1.2395e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2302e-05 - val_loss: 5.7112e-06\n",
            "Epoch 17/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.3367e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3341e-05 - val_loss: 3.0995e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 1.0\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_62 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_122 (Dense)              (None, 7)            49          ['input_62[0][0]']               \n",
            "                                                                                                  \n",
            " dense_123 (Dense)              (None, 7)            49          ['dense_122[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_61 (TFOpLambda)   (None, 7)            0           ['dense_123[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_61 (TFOpLambd  (None, 7)           0           ['tf.ones_like_61[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_61 (TFOpL  (None, 7)           0           ['tf.math.multiply_61[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_61 (TFOpLambda)        (None, 7)            0           ['dense_123[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_61   (None, 7)           0           ['tf.convert_to_tensor_61[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_61[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_122 (TFOpL  (None,)             0           ['tf.math.squared_difference_61[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_123 (TFOpL  ()                  0           ['tf.math.reduce_mean_122[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_61 (AddLoss)          ()                   0           ['tf.math.reduce_mean_123[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/61/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0019 - val_loss: 9.9776e-06\n",
            "Epoch 2/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 3.2345e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00001, saving model to results/61/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.2468e-05 - val_loss: 7.9903e-06\n",
            "Epoch 3/500\n",
            "4200/4236 [============================>.] - ETA: 0s - loss: 2.6404e-05\n",
            "Epoch 3: val_loss improved from 0.00001 to 0.00001, saving model to results/61/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6221e-05 - val_loss: 5.1735e-06\n",
            "Epoch 4/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 2.3987e-05\n",
            "Epoch 4: val_loss improved from 0.00001 to 0.00000, saving model to results/61/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3978e-05 - val_loss: 4.8249e-06\n",
            "Epoch 5/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.8153e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.8266e-05 - val_loss: 9.3762e-05\n",
            "Epoch 6/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.3830e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3880e-05 - val_loss: 1.5397e-05\n",
            "Epoch 7/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.3243e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3229e-05 - val_loss: 5.4731e-06\n",
            "Epoch 8/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.3323e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3489e-05 - val_loss: 5.4364e-06\n",
            "Epoch 9/500\n",
            "4199/4236 [============================>.] - ETA: 0s - loss: 1.3077e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3080e-05 - val_loss: 5.7031e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.951367048725868\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_63 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_124 (Dense)              (None, 7)            49          ['input_63[0][0]']               \n",
            "                                                                                                  \n",
            " dense_125 (Dense)              (None, 7)            49          ['dense_124[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_62 (TFOpLambda)   (None, 7)            0           ['dense_125[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_62 (TFOpLambd  (None, 7)           0           ['tf.ones_like_62[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_62 (TFOpL  (None, 7)           0           ['tf.math.multiply_62[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_62 (TFOpLambda)        (None, 7)            0           ['dense_125[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_62   (None, 7)           0           ['tf.convert_to_tensor_62[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_62[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_124 (TFOpL  (None,)             0           ['tf.math.squared_difference_62[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_125 (TFOpL  ()                  0           ['tf.math.reduce_mean_124[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_62 (AddLoss)          ()                   0           ['tf.math.reduce_mean_125[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00009, saving model to results/62/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0018 - val_loss: 9.4378e-05\n",
            "Epoch 2/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 3.9421e-05\n",
            "Epoch 2: val_loss did not improve from 0.00009\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.9418e-05 - val_loss: 3.0319e-04\n",
            "Epoch 3/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 2.6850e-05\n",
            "Epoch 3: val_loss improved from 0.00009 to 0.00000, saving model to results/62/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.6681e-05 - val_loss: 1.3582e-06\n",
            "Epoch 4/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.7972e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7972e-05 - val_loss: 0.0015\n",
            "Epoch 5/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.4232e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/62/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4126e-05 - val_loss: 1.0560e-07\n",
            "Epoch 6/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.3706e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/62/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3677e-05 - val_loss: 1.1611e-09\n",
            "Epoch 7/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.3785e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3739e-05 - val_loss: 1.2013e-09\n",
            "Epoch 8/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.4637e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4570e-05 - val_loss: 2.5758e-06\n",
            "Epoch 9/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.2556e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3241e-05 - val_loss: 1.8470e-04\n",
            "Epoch 10/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 1.2982e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3268e-05 - val_loss: 1.0780e-04\n",
            "Epoch 11/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.3867e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3812e-05 - val_loss: 2.0257e-08\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5000842964511203\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_64 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_126 (Dense)              (None, 7)            49          ['input_64[0][0]']               \n",
            "                                                                                                  \n",
            " dense_127 (Dense)              (None, 7)            49          ['dense_126[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_63 (TFOpLambda)   (None, 7)            0           ['dense_127[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_63 (TFOpLambd  (None, 7)           0           ['tf.ones_like_63[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_63 (TFOpL  (None, 7)           0           ['tf.math.multiply_63[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_63 (TFOpLambda)        (None, 7)            0           ['dense_127[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_63   (None, 7)           0           ['tf.convert_to_tensor_63[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_63[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_126 (TFOpL  (None,)             0           ['tf.math.squared_difference_63[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_127 (TFOpL  ()                  0           ['tf.math.reduce_mean_126[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_63 (AddLoss)          ()                   0           ['tf.math.reduce_mean_127[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00015, saving model to results/63/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0018 - val_loss: 1.5031e-04\n",
            "Epoch 2/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 6.5807e-05\n",
            "Epoch 2: val_loss improved from 0.00015 to 0.00001, saving model to results/63/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 6.5792e-05 - val_loss: 5.8780e-06\n",
            "Epoch 3/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 3.8919e-05\n",
            "Epoch 3: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.9289e-05 - val_loss: 7.2665e-05\n",
            "Epoch 4/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 2.5380e-05\n",
            "Epoch 4: val_loss improved from 0.00001 to 0.00000, saving model to results/63/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.5343e-05 - val_loss: 3.4368e-06\n",
            "Epoch 5/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 3.3039e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/63/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 3.2917e-05 - val_loss: 1.9086e-06\n",
            "Epoch 6/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.6895e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/63/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6888e-05 - val_loss: 1.3605e-06\n",
            "Epoch 7/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.7306e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7288e-05 - val_loss: 4.7273e-06\n",
            "Epoch 8/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.5582e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5594e-05 - val_loss: 5.3366e-05\n",
            "Epoch 9/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.5785e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5769e-05 - val_loss: 2.6019e-06\n",
            "Epoch 10/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 1.4055e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4280e-05 - val_loss: 1.0760e-04\n",
            "Epoch 11/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.5159e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5158e-05 - val_loss: 1.0165e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8462231631496151\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_65 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_128 (Dense)              (None, 7)            49          ['input_65[0][0]']               \n",
            "                                                                                                  \n",
            " dense_129 (Dense)              (None, 7)            49          ['dense_128[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_64 (TFOpLambda)   (None, 7)            0           ['dense_129[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_64 (TFOpLambd  (None, 7)           0           ['tf.ones_like_64[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_64 (TFOpL  (None, 7)           0           ['tf.math.multiply_64[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_64 (TFOpLambda)        (None, 7)            0           ['dense_129[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_64   (None, 7)           0           ['tf.convert_to_tensor_64[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_64[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_128 (TFOpL  (None,)             0           ['tf.math.squared_difference_64[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_129 (TFOpL  ()                  0           ['tf.math.reduce_mean_128[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_64 (AddLoss)          ()                   0           ['tf.math.reduce_mean_129[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/64/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0024 - val_loss: 1.1280e-05\n",
            "Epoch 2/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 3.6401e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00000, saving model to results/64/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 3.6293e-05 - val_loss: 2.2933e-06\n",
            "Epoch 3/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 2.8887e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/64/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.8792e-05 - val_loss: 1.1018e-07\n",
            "Epoch 4/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 2.0101e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.0040e-05 - val_loss: 1.2307e-07\n",
            "Epoch 5/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.4716e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/64/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4630e-05 - val_loss: 1.0472e-07\n",
            "Epoch 6/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.3414e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3316e-05 - val_loss: 1.2098e-07\n",
            "Epoch 7/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 1.4885e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4787e-05 - val_loss: 1.6156e-07\n",
            "Epoch 8/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.5112e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5066e-05 - val_loss: 1.0878e-07\n",
            "Epoch 9/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.3552e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3478e-05 - val_loss: 1.1027e-07\n",
            "Epoch 10/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.4303e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4256e-05 - val_loss: 1.1169e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5234573503269124\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_66 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_130 (Dense)              (None, 7)            49          ['input_66[0][0]']               \n",
            "                                                                                                  \n",
            " dense_131 (Dense)              (None, 7)            49          ['dense_130[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_65 (TFOpLambda)   (None, 7)            0           ['dense_131[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_65 (TFOpLambd  (None, 7)           0           ['tf.ones_like_65[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_65 (TFOpL  (None, 7)           0           ['tf.math.multiply_65[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_65 (TFOpLambda)        (None, 7)            0           ['dense_131[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_65   (None, 7)           0           ['tf.convert_to_tensor_65[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_65[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_130 (TFOpL  (None,)             0           ['tf.math.squared_difference_65[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_131 (TFOpL  ()                  0           ['tf.math.reduce_mean_130[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_65 (AddLoss)          ()                   0           ['tf.math.reduce_mean_131[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/65/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0021 - val_loss: 2.7886e-09\n",
            "Epoch 2/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 2.3717e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3583e-05 - val_loss: 2.9323e-09\n",
            "Epoch 3/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.2633e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.2627e-05 - val_loss: 8.1474e-08\n",
            "Epoch 4/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.8799e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8746e-05 - val_loss: 2.7615e-08\n",
            "Epoch 5/500\n",
            "4200/4236 [============================>.] - ETA: 0s - loss: 1.3364e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3251e-05 - val_loss: 1.2612e-08\n",
            "Epoch 6/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.1826e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1779e-05 - val_loss: 6.4252e-08\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.6457306093535103\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_67 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_132 (Dense)              (None, 7)            49          ['input_67[0][0]']               \n",
            "                                                                                                  \n",
            " dense_133 (Dense)              (None, 7)            49          ['dense_132[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_66 (TFOpLambda)   (None, 7)            0           ['dense_133[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_66 (TFOpLambd  (None, 7)           0           ['tf.ones_like_66[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_66 (TFOpL  (None, 7)           0           ['tf.math.multiply_66[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_66 (TFOpLambda)        (None, 7)            0           ['dense_133[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_66   (None, 7)           0           ['tf.convert_to_tensor_66[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_66[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_132 (TFOpL  (None,)             0           ['tf.math.squared_difference_66[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_133 (TFOpL  ()                  0           ['tf.math.reduce_mean_132[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_66 (AddLoss)          ()                   0           ['tf.math.reduce_mean_133[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 0.0016\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/66/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0016 - val_loss: 1.4486e-06\n",
            "Epoch 2/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 4.3854e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/66/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.3854e-05 - val_loss: 5.6947e-08\n",
            "Epoch 3/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 2.0222e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0168e-05 - val_loss: 9.9088e-05\n",
            "Epoch 4/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.4499e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4570e-05 - val_loss: 0.0011\n",
            "Epoch 5/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.3105e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3108e-05 - val_loss: 8.7664e-05\n",
            "Epoch 6/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.3153e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/66/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3113e-05 - val_loss: 6.7483e-10\n",
            "Epoch 7/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.2397e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2337e-05 - val_loss: 1.3607e-06\n",
            "Epoch 8/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.4075e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4065e-05 - val_loss: 1.0354e-09\n",
            "Epoch 9/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.2120e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2083e-05 - val_loss: 3.9837e-07\n",
            "Epoch 10/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.3594e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3539e-05 - val_loss: 1.2076e-09\n",
            "Epoch 11/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.2382e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3159e-05 - val_loss: 1.1126e-04\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.43937395852522276\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_68 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_134 (Dense)              (None, 7)            49          ['input_68[0][0]']               \n",
            "                                                                                                  \n",
            " dense_135 (Dense)              (None, 7)            49          ['dense_134[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_67 (TFOpLambda)   (None, 7)            0           ['dense_135[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_67 (TFOpLambd  (None, 7)           0           ['tf.ones_like_67[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_67 (TFOpL  (None, 7)           0           ['tf.math.multiply_67[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_67 (TFOpLambda)        (None, 7)            0           ['dense_135[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_67   (None, 7)           0           ['tf.convert_to_tensor_67[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_67[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_134 (TFOpL  (None,)             0           ['tf.math.squared_difference_67[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_135 (TFOpL  ()                  0           ['tf.math.reduce_mean_134[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_67 (AddLoss)          ()                   0           ['tf.math.reduce_mean_135[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00012, saving model to results/67/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0017 - val_loss: 1.2212e-04\n",
            "Epoch 2/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 4.8598e-05\n",
            "Epoch 2: val_loss improved from 0.00012 to 0.00001, saving model to results/67/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.8424e-05 - val_loss: 8.8392e-06\n",
            "Epoch 3/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 3.2829e-05\n",
            "Epoch 3: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.2783e-05 - val_loss: 9.2738e-06\n",
            "Epoch 4/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 2.4835e-05\n",
            "Epoch 4: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4700e-05 - val_loss: 9.0630e-06\n",
            "Epoch 5/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.2183e-05\n",
            "Epoch 5: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.2083e-05 - val_loss: 9.0928e-06\n",
            "Epoch 6/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.4579e-05\n",
            "Epoch 6: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4573e-05 - val_loss: 9.0720e-06\n",
            "Epoch 7/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.3719e-05\n",
            "Epoch 7: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3664e-05 - val_loss: 8.9090e-06\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.47462849825812975\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_69 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_136 (Dense)              (None, 7)            49          ['input_69[0][0]']               \n",
            "                                                                                                  \n",
            " dense_137 (Dense)              (None, 7)            49          ['dense_136[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_68 (TFOpLambda)   (None, 7)            0           ['dense_137[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_68 (TFOpLambd  (None, 7)           0           ['tf.ones_like_68[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_68 (TFOpL  (None, 7)           0           ['tf.math.multiply_68[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_68 (TFOpLambda)        (None, 7)            0           ['dense_137[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_68   (None, 7)           0           ['tf.convert_to_tensor_68[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_68[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_136 (TFOpL  (None,)             0           ['tf.math.squared_difference_68[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_137 (TFOpL  ()                  0           ['tf.math.reduce_mean_136[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_68 (AddLoss)          ()                   0           ['tf.math.reduce_mean_137[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/68/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0018 - val_loss: 1.2591e-06\n",
            "Epoch 2/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 3.4118e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/68/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.4157e-05 - val_loss: 9.4299e-07\n",
            "Epoch 3/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 3.2537e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.2724e-05 - val_loss: 6.3419e-05\n",
            "Epoch 4/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 2.4188e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/68/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4177e-05 - val_loss: 6.2652e-07\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.3729e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3735e-05 - val_loss: 4.4296e-05\n",
            "Epoch 6/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.9320e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/68/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9230e-05 - val_loss: 3.5831e-07\n",
            "Epoch 7/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.4489e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4491e-05 - val_loss: 8.9668e-06\n",
            "Epoch 8/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.2339e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/68/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2246e-05 - val_loss: 2.8297e-09\n",
            "Epoch 9/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.1585e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1520e-05 - val_loss: 1.1675e-07\n",
            "Epoch 10/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.2330e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2290e-05 - val_loss: 8.2775e-08\n",
            "Epoch 11/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.2041e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1962e-05 - val_loss: 1.7918e-08\n",
            "Epoch 12/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.1788e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.1785e-05 - val_loss: 2.8868e-08\n",
            "Epoch 13/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.2109e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2103e-05 - val_loss: 4.3400e-08\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5611768004290285\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_70 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_138 (Dense)              (None, 7)            49          ['input_70[0][0]']               \n",
            "                                                                                                  \n",
            " dense_139 (Dense)              (None, 7)            49          ['dense_138[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_69 (TFOpLambda)   (None, 7)            0           ['dense_139[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_69 (TFOpLambd  (None, 7)           0           ['tf.ones_like_69[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_69 (TFOpL  (None, 7)           0           ['tf.math.multiply_69[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_69 (TFOpLambda)        (None, 7)            0           ['dense_139[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_69   (None, 7)           0           ['tf.convert_to_tensor_69[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_69[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_138 (TFOpL  (None,)             0           ['tf.math.squared_difference_69[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_139 (TFOpL  ()                  0           ['tf.math.reduce_mean_138[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_69 (AddLoss)          ()                   0           ['tf.math.reduce_mean_139[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/69/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0024 - val_loss: 1.0177e-08\n",
            "Epoch 2/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.7900e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7855e-05 - val_loss: 1.4016e-06\n",
            "Epoch 3/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.9633e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9527e-05 - val_loss: 1.8540e-06\n",
            "Epoch 4/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.8353e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8297e-05 - val_loss: 4.5326e-07\n",
            "Epoch 5/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.5040e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5124e-05 - val_loss: 2.6012e-04\n",
            "Epoch 6/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.1612e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/69/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.1601e-05 - val_loss: 4.5516e-09\n",
            "Epoch 7/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.0022e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0006e-05 - val_loss: 6.2036e-06\n",
            "Epoch 8/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.0569e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0596e-05 - val_loss: 5.7289e-05\n",
            "Epoch 9/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 9.8314e-06\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 9.8092e-06 - val_loss: 3.9175e-06\n",
            "Epoch 10/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.0208e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0195e-05 - val_loss: 4.8689e-06\n",
            "Epoch 11/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.0159e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0541e-05 - val_loss: 7.4801e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.6016980331179558\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_71 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_140 (Dense)              (None, 7)            49          ['input_71[0][0]']               \n",
            "                                                                                                  \n",
            " dense_141 (Dense)              (None, 7)            49          ['dense_140[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_70 (TFOpLambda)   (None, 7)            0           ['dense_141[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_70 (TFOpLambd  (None, 7)           0           ['tf.ones_like_70[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_70 (TFOpL  (None, 7)           0           ['tf.math.multiply_70[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_70 (TFOpLambda)        (None, 7)            0           ['dense_141[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_70   (None, 7)           0           ['tf.convert_to_tensor_70[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_70[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_140 (TFOpL  (None,)             0           ['tf.math.squared_difference_70[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_141 (TFOpL  ()                  0           ['tf.math.reduce_mean_140[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_70 (AddLoss)          ()                   0           ['tf.math.reduce_mean_141[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/70/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0024 - val_loss: 3.1017e-09\n",
            "Epoch 2/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.6514e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6521e-05 - val_loss: 2.9863e-05\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.2380e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2325e-05 - val_loss: 2.2582e-07\n",
            "Epoch 4/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 1.2983e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2875e-05 - val_loss: 3.8624e-09\n",
            "Epoch 5/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.2400e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2400e-05 - val_loss: 5.7132e-07\n",
            "Epoch 6/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.1274e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1197e-05 - val_loss: 1.5043e-08\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5933178096351478\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_72 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_142 (Dense)              (None, 7)            49          ['input_72[0][0]']               \n",
            "                                                                                                  \n",
            " dense_143 (Dense)              (None, 7)            49          ['dense_142[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_71 (TFOpLambda)   (None, 7)            0           ['dense_143[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_71 (TFOpLambd  (None, 7)           0           ['tf.ones_like_71[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_71 (TFOpL  (None, 7)           0           ['tf.math.multiply_71[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_71 (TFOpLambda)        (None, 7)            0           ['dense_143[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_71   (None, 7)           0           ['tf.convert_to_tensor_71[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_71[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_142 (TFOpL  (None,)             0           ['tf.math.squared_difference_71[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_143 (TFOpL  ()                  0           ['tf.math.reduce_mean_142[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_71 (AddLoss)          ()                   0           ['tf.math.reduce_mean_143[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/71/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0017 - val_loss: 1.9082e-06\n",
            "Epoch 2/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 3.4126e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4036e-05 - val_loss: 2.0740e-05\n",
            "Epoch 3/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 3.4780e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4773e-05 - val_loss: 5.0693e-06\n",
            "Epoch 4/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 3.4580e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/71/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4453e-05 - val_loss: 1.7671e-06\n",
            "Epoch 5/500\n",
            "4198/4236 [============================>.] - ETA: 0s - loss: 3.2830e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.3456e-05 - val_loss: 4.6456e-05\n",
            "Epoch 6/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 3.3370e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.3413e-05 - val_loss: 9.1807e-06\n",
            "Epoch 7/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.8751e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.8769e-05 - val_loss: 1.4127e-04\n",
            "Epoch 8/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.7860e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7843e-05 - val_loss: 5.9533e-06\n",
            "Epoch 9/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.7048e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7282e-05 - val_loss: 2.6691e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999991130844422\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_73 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_144 (Dense)              (None, 7)            49          ['input_73[0][0]']               \n",
            "                                                                                                  \n",
            " dense_145 (Dense)              (None, 7)            49          ['dense_144[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_72 (TFOpLambda)   (None, 7)            0           ['dense_145[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_72 (TFOpLambd  (None, 7)           0           ['tf.ones_like_72[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_72 (TFOpL  (None, 7)           0           ['tf.math.multiply_72[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_72 (TFOpLambda)        (None, 7)            0           ['dense_145[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_72   (None, 7)           0           ['tf.convert_to_tensor_72[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_72[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_144 (TFOpL  (None,)             0           ['tf.math.squared_difference_72[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_145 (TFOpL  ()                  0           ['tf.math.reduce_mean_144[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_72 (AddLoss)          ()                   0           ['tf.math.reduce_mean_145[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00020, saving model to results/72/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0019 - val_loss: 2.0157e-04\n",
            "Epoch 2/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 3.3331e-05\n",
            "Epoch 2: val_loss did not improve from 0.00020\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.3604e-05 - val_loss: 0.0014\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.7837e-05\n",
            "Epoch 3: val_loss improved from 0.00020 to 0.00000, saving model to results/72/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.7713e-05 - val_loss: 4.9685e-09\n",
            "Epoch 4/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.5547e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.6135e-05 - val_loss: 7.9256e-05\n",
            "Epoch 5/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 2.4667e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4871e-05 - val_loss: 2.5120e-05\n",
            "Epoch 6/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 2.3997e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/72/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3929e-05 - val_loss: 3.4214e-09\n",
            "Epoch 7/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.4072e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.4066e-05 - val_loss: 3.7907e-09\n",
            "Epoch 8/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.5100e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4997e-05 - val_loss: 2.8380e-08\n",
            "Epoch 9/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.3247e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3268e-05 - val_loss: 1.0018e-05\n",
            "Epoch 10/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.2607e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2548e-05 - val_loss: 1.5703e-08\n",
            "Epoch 11/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.3523e-05\n",
            "Epoch 11: val_loss improved from 0.00000 to 0.00000, saving model to results/72/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3469e-05 - val_loss: 2.7623e-09\n",
            "Epoch 12/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.2927e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2856e-05 - val_loss: 1.2099e-06\n",
            "Epoch 13/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2753e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2700e-05 - val_loss: 1.3455e-06\n",
            "Epoch 14/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.4046e-05\n",
            "Epoch 14: val_loss improved from 0.00000 to 0.00000, saving model to results/72/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3997e-05 - val_loss: 8.0991e-10\n",
            "Epoch 15/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.1919e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2133e-05 - val_loss: 3.3195e-04\n",
            "Epoch 16/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2732e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2729e-05 - val_loss: 8.9321e-10\n",
            "Epoch 17/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.2691e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2658e-05 - val_loss: 3.3168e-09\n",
            "Epoch 18/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3081e-05\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3078e-05 - val_loss: 9.4204e-08\n",
            "Epoch 19/500\n",
            "4199/4236 [============================>.] - ETA: 0s - loss: 1.1987e-05\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2666e-05 - val_loss: 8.6874e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5812121541174944\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_74 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_146 (Dense)              (None, 7)            49          ['input_74[0][0]']               \n",
            "                                                                                                  \n",
            " dense_147 (Dense)              (None, 7)            49          ['dense_146[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_73 (TFOpLambda)   (None, 7)            0           ['dense_147[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_73 (TFOpLambd  (None, 7)           0           ['tf.ones_like_73[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_73 (TFOpL  (None, 7)           0           ['tf.math.multiply_73[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_73 (TFOpLambda)        (None, 7)            0           ['dense_147[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_73   (None, 7)           0           ['tf.convert_to_tensor_73[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_73[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_146 (TFOpL  (None,)             0           ['tf.math.squared_difference_73[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_147 (TFOpL  ()                  0           ['tf.math.reduce_mean_146[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_73 (AddLoss)          ()                   0           ['tf.math.reduce_mean_147[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 0.0015\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to results/73/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0015 - val_loss: 4.3369e-05\n",
            "Epoch 2/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 5.4387e-05\n",
            "Epoch 2: val_loss improved from 0.00004 to 0.00000, saving model to results/73/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 5.4248e-05 - val_loss: 7.8209e-07\n",
            "Epoch 3/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 4.4315e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.4317e-05 - val_loss: 4.0812e-05\n",
            "Epoch 4/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.8378e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.8378e-05 - val_loss: 2.0343e-06\n",
            "Epoch 5/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 2.3083e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2949e-05 - val_loss: 8.2490e-07\n",
            "Epoch 6/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.8931e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/73/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8887e-05 - val_loss: 2.2773e-07\n",
            "Epoch 7/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.4365e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4355e-05 - val_loss: 1.9515e-06\n",
            "Epoch 8/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.2864e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/73/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2843e-05 - val_loss: 7.3678e-08\n",
            "Epoch 9/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.1976e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/73/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1925e-05 - val_loss: 4.7305e-10\n",
            "Epoch 10/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.2203e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2153e-05 - val_loss: 2.6751e-07\n",
            "Epoch 11/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.2700e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2624e-05 - val_loss: 2.0680e-07\n",
            "Epoch 12/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.2708e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2696e-05 - val_loss: 2.0613e-09\n",
            "Epoch 13/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.2711e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2636e-05 - val_loss: 4.8131e-10\n",
            "Epoch 14/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.1832e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1818e-05 - val_loss: 1.0871e-09\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999933323901209\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_75 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_148 (Dense)              (None, 7)            49          ['input_75[0][0]']               \n",
            "                                                                                                  \n",
            " dense_149 (Dense)              (None, 7)            49          ['dense_148[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_74 (TFOpLambda)   (None, 7)            0           ['dense_149[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_74 (TFOpLambd  (None, 7)           0           ['tf.ones_like_74[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_74 (TFOpL  (None, 7)           0           ['tf.math.multiply_74[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_74 (TFOpLambda)        (None, 7)            0           ['dense_149[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_74   (None, 7)           0           ['tf.convert_to_tensor_74[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_74[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_148 (TFOpL  (None,)             0           ['tf.math.squared_difference_74[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_149 (TFOpL  ()                  0           ['tf.math.reduce_mean_148[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_74 (AddLoss)          ()                   0           ['tf.math.reduce_mean_149[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/74/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0018 - val_loss: 1.9223e-06\n",
            "Epoch 2/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 4.8155e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.8130e-05 - val_loss: 1.1644e-05\n",
            "Epoch 3/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.6877e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/74/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.6816e-05 - val_loss: 7.2509e-10\n",
            "Epoch 4/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 2.6603e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.6388e-05 - val_loss: 2.1328e-05\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.6063e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.6056e-05 - val_loss: 8.7459e-10\n",
            "Epoch 6/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 2.2736e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2688e-05 - val_loss: 1.3733e-08\n",
            "Epoch 7/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.8945e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8887e-05 - val_loss: 6.4029e-08\n",
            "Epoch 8/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.2625e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/74/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2574e-05 - val_loss: 1.6120e-12\n",
            "Epoch 9/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 1.3902e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3883e-05 - val_loss: 2.4708e-10\n",
            "Epoch 10/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.2170e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2138e-05 - val_loss: 3.2721e-08\n",
            "Epoch 11/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.1841e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1841e-05 - val_loss: 2.5367e-07\n",
            "Epoch 12/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2926e-05\n",
            "Epoch 12: val_loss improved from 0.00000 to 0.00000, saving model to results/74/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2853e-05 - val_loss: 5.6946e-14\n",
            "Epoch 13/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.2773e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2736e-05 - val_loss: 1.0880e-09\n",
            "Epoch 14/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.2914e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2838e-05 - val_loss: 8.7983e-08\n",
            "Epoch 15/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2840e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2785e-05 - val_loss: 9.4414e-14\n",
            "Epoch 16/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2937e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2934e-05 - val_loss: 9.7884e-06\n",
            "Epoch 17/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.2413e-05\n",
            "Epoch 17: val_loss improved from 0.00000 to 0.00000, saving model to results/74/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2337e-05 - val_loss: 4.5232e-14\n",
            "Epoch 18/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2828e-05\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2774e-05 - val_loss: 1.3171e-07\n",
            "Epoch 19/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.2929e-05\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2905e-05 - val_loss: 2.8596e-10\n",
            "Epoch 20/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.2832e-05\n",
            "Epoch 20: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3086e-05 - val_loss: 2.8134e-05\n",
            "Epoch 21/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2269e-05\n",
            "Epoch 21: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2557e-05 - val_loss: 1.5871e-06\n",
            "Epoch 22/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.2257e-05\n",
            "Epoch 22: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2196e-05 - val_loss: 3.5580e-10\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 1.0\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_76 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_150 (Dense)              (None, 7)            49          ['input_76[0][0]']               \n",
            "                                                                                                  \n",
            " dense_151 (Dense)              (None, 7)            49          ['dense_150[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_75 (TFOpLambda)   (None, 7)            0           ['dense_151[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_75 (TFOpLambd  (None, 7)           0           ['tf.ones_like_75[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_75 (TFOpL  (None, 7)           0           ['tf.math.multiply_75[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_75 (TFOpLambda)        (None, 7)            0           ['dense_151[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_75   (None, 7)           0           ['tf.convert_to_tensor_75[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_75[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_150 (TFOpL  (None,)             0           ['tf.math.squared_difference_75[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_151 (TFOpL  ()                  0           ['tf.math.reduce_mean_150[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_75 (AddLoss)          ()                   0           ['tf.math.reduce_mean_151[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/75/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0019 - val_loss: 1.5032e-08\n",
            "Epoch 2/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.0110e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0060e-05 - val_loss: 1.4737e-07\n",
            "Epoch 3/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 2.7130e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7054e-05 - val_loss: 1.1367e-07\n",
            "Epoch 4/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 2.3336e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3182e-05 - val_loss: 3.8164e-06\n",
            "Epoch 5/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 2.0719e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/75/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0640e-05 - val_loss: 1.8045e-09\n",
            "Epoch 6/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 2.1090e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.1036e-05 - val_loss: 1.0366e-08\n",
            "Epoch 7/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.9217e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9113e-05 - val_loss: 3.8651e-08\n",
            "Epoch 8/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.7705e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7705e-05 - val_loss: 1.1960e-07\n",
            "Epoch 9/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.1217e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1197e-05 - val_loss: 1.7527e-08\n",
            "Epoch 10/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 9.9898e-06\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 9.9508e-06 - val_loss: 1.5976e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.46034252300501555\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_77 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_152 (Dense)              (None, 7)            49          ['input_77[0][0]']               \n",
            "                                                                                                  \n",
            " dense_153 (Dense)              (None, 7)            49          ['dense_152[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_76 (TFOpLambda)   (None, 7)            0           ['dense_153[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_76 (TFOpLambd  (None, 7)           0           ['tf.ones_like_76[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_76 (TFOpL  (None, 7)           0           ['tf.math.multiply_76[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_76 (TFOpLambda)        (None, 7)            0           ['dense_153[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_76   (None, 7)           0           ['tf.convert_to_tensor_76[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_76[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_152 (TFOpL  (None,)             0           ['tf.math.squared_difference_76[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_153 (TFOpL  ()                  0           ['tf.math.reduce_mean_152[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_76 (AddLoss)          ()                   0           ['tf.math.reduce_mean_153[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00623, saving model to results/76/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0024 - val_loss: 0.0062\n",
            "Epoch 2/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 2.5139e-05\n",
            "Epoch 2: val_loss did not improve from 0.00623\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5893e-05 - val_loss: 0.0079\n",
            "Epoch 3/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.8531e-05\n",
            "Epoch 3: val_loss improved from 0.00623 to 0.00139, saving model to results/76/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8617e-05 - val_loss: 0.0014\n",
            "Epoch 4/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.2323e-05\n",
            "Epoch 4: val_loss improved from 0.00139 to 0.00000, saving model to results/76/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2303e-05 - val_loss: 1.0265e-06\n",
            "Epoch 5/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.1120e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/76/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1062e-05 - val_loss: 7.4062e-10\n",
            "Epoch 6/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.0439e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0412e-05 - val_loss: 1.2853e-08\n",
            "Epoch 7/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.0311e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0648e-05 - val_loss: 2.0267e-05\n",
            "Epoch 8/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.0611e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0566e-05 - val_loss: 7.4105e-10\n",
            "Epoch 9/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.0174e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0143e-05 - val_loss: 2.2538e-09\n",
            "Epoch 10/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.0302e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0243e-05 - val_loss: 3.1202e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.638847024921876\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_78 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_154 (Dense)              (None, 7)            49          ['input_78[0][0]']               \n",
            "                                                                                                  \n",
            " dense_155 (Dense)              (None, 7)            49          ['dense_154[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_77 (TFOpLambda)   (None, 7)            0           ['dense_155[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_77 (TFOpLambd  (None, 7)           0           ['tf.ones_like_77[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_77 (TFOpL  (None, 7)           0           ['tf.math.multiply_77[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_77 (TFOpLambda)        (None, 7)            0           ['dense_155[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_77   (None, 7)           0           ['tf.convert_to_tensor_77[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_77[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_154 (TFOpL  (None,)             0           ['tf.math.squared_difference_77[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_155 (TFOpL  ()                  0           ['tf.math.reduce_mean_154[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_77 (AddLoss)          ()                   0           ['tf.math.reduce_mean_155[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/77/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0021 - val_loss: 2.2294e-07\n",
            "Epoch 2/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 3.4789e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4716e-05 - val_loss: 1.8146e-05\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.6037e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5966e-05 - val_loss: 4.3179e-07\n",
            "Epoch 4/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2057e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2054e-05 - val_loss: 2.2397e-06\n",
            "Epoch 5/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.1403e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1373e-05 - val_loss: 4.4177e-07\n",
            "Epoch 6/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.2278e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/77/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2245e-05 - val_loss: 1.1932e-07\n",
            "Epoch 7/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.1881e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1818e-05 - val_loss: 5.9231e-07\n",
            "Epoch 8/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.2635e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2586e-05 - val_loss: 1.6052e-06\n",
            "Epoch 9/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.1592e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1559e-05 - val_loss: 1.0055e-05\n",
            "Epoch 10/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.1891e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1886e-05 - val_loss: 7.2543e-06\n",
            "Epoch 11/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.2204e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2147e-05 - val_loss: 4.9770e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5327330957352019\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_79 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_156 (Dense)              (None, 7)            49          ['input_79[0][0]']               \n",
            "                                                                                                  \n",
            " dense_157 (Dense)              (None, 7)            49          ['dense_156[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_78 (TFOpLambda)   (None, 7)            0           ['dense_157[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_78 (TFOpLambd  (None, 7)           0           ['tf.ones_like_78[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_78 (TFOpL  (None, 7)           0           ['tf.math.multiply_78[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_78 (TFOpLambda)        (None, 7)            0           ['dense_157[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_78   (None, 7)           0           ['tf.convert_to_tensor_78[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_78[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_156 (TFOpL  (None,)             0           ['tf.math.squared_difference_78[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_157 (TFOpL  ()                  0           ['tf.math.reduce_mean_156[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_78 (AddLoss)          ()                   0           ['tf.math.reduce_mean_157[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 0.0041\n",
            "Epoch 1: val_loss improved from inf to 0.00094, saving model to results/78/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0040 - val_loss: 9.3530e-04\n",
            "Epoch 2/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 8.1345e-04\n",
            "Epoch 2: val_loss improved from 0.00094 to 0.00015, saving model to results/78/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 8.1339e-04 - val_loss: 1.5376e-04\n",
            "Epoch 3/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 3.9189e-05\n",
            "Epoch 3: val_loss improved from 0.00015 to 0.00000, saving model to results/78/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.9135e-05 - val_loss: 2.2376e-06\n",
            "Epoch 4/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 3.6147e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/78/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.6007e-05 - val_loss: 5.7328e-07\n",
            "Epoch 5/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 0.0013\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 6/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 0.0033\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0033 - val_loss: 0.0025\n",
            "Epoch 7/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 0.0033\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0033 - val_loss: 0.0013\n",
            "Epoch 8/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0033\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0033 - val_loss: 8.8752e-04\n",
            "Epoch 9/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 0.0033\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "17649/17649 [==============================] - 24s 1ms/step\n",
            "17649/17649 [==============================] - 24s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.7761669793935528\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_80 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_158 (Dense)              (None, 7)            49          ['input_80[0][0]']               \n",
            "                                                                                                  \n",
            " dense_159 (Dense)              (None, 7)            49          ['dense_158[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_79 (TFOpLambda)   (None, 7)            0           ['dense_159[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_79 (TFOpLambd  (None, 7)           0           ['tf.ones_like_79[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_79 (TFOpL  (None, 7)           0           ['tf.math.multiply_79[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_79 (TFOpLambda)        (None, 7)            0           ['dense_159[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_79   (None, 7)           0           ['tf.convert_to_tensor_79[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_79[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_158 (TFOpL  (None,)             0           ['tf.math.squared_difference_79[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_159 (TFOpL  ()                  0           ['tf.math.reduce_mean_158[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_79 (AddLoss)          ()                   0           ['tf.math.reduce_mean_159[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to results/79/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0017 - val_loss: 3.3414e-05\n",
            "Epoch 2/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 3.9418e-05\n",
            "Epoch 2: val_loss improved from 0.00003 to 0.00000, saving model to results/79/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.9260e-05 - val_loss: 2.8999e-07\n",
            "Epoch 3/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 3.5472e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.5188e-05 - val_loss: 3.9663e-07\n",
            "Epoch 4/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 2.6411e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6408e-05 - val_loss: 9.6199e-06\n",
            "Epoch 5/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.6448e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6402e-05 - val_loss: 7.4419e-07\n",
            "Epoch 6/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.2667e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2605e-05 - val_loss: 3.0775e-07\n",
            "Epoch 7/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.2516e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2433e-05 - val_loss: 3.4852e-07\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5148600750237291\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_81 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_160 (Dense)              (None, 7)            49          ['input_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_161 (Dense)              (None, 7)            49          ['dense_160[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_80 (TFOpLambda)   (None, 7)            0           ['dense_161[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_80 (TFOpLambd  (None, 7)           0           ['tf.ones_like_80[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_80 (TFOpL  (None, 7)           0           ['tf.math.multiply_80[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_80 (TFOpLambda)        (None, 7)            0           ['dense_161[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_80   (None, 7)           0           ['tf.convert_to_tensor_80[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_80[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_160 (TFOpL  (None,)             0           ['tf.math.squared_difference_80[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_161 (TFOpL  ()                  0           ['tf.math.reduce_mean_160[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_80 (AddLoss)          ()                   0           ['tf.math.reduce_mean_161[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 0.0024\n",
            "Epoch 1: val_loss improved from inf to 0.00022, saving model to results/80/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0024 - val_loss: 2.2007e-04\n",
            "Epoch 2/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.8742e-05\n",
            "Epoch 2: val_loss improved from 0.00022 to 0.00022, saving model to results/80/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.8742e-05 - val_loss: 2.1875e-04\n",
            "Epoch 3/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.4403e-05\n",
            "Epoch 3: val_loss improved from 0.00022 to 0.00018, saving model to results/80/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4343e-05 - val_loss: 1.8138e-04\n",
            "Epoch 4/500\n",
            "4197/4236 [============================>.] - ETA: 0s - loss: 1.3661e-05\n",
            "Epoch 4: val_loss did not improve from 0.00018\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3536e-05 - val_loss: 2.2080e-04\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3032e-05\n",
            "Epoch 5: val_loss did not improve from 0.00018\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3029e-05 - val_loss: 2.0652e-04\n",
            "Epoch 6/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.3979e-05\n",
            "Epoch 6: val_loss did not improve from 0.00018\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3893e-05 - val_loss: 2.2089e-04\n",
            "Epoch 7/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.2251e-05\n",
            "Epoch 7: val_loss did not improve from 0.00018\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2193e-05 - val_loss: 2.2005e-04\n",
            "Epoch 8/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.3321e-05\n",
            "Epoch 8: val_loss did not improve from 0.00018\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3224e-05 - val_loss: 2.2072e-04\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.3972360111785453\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_82 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_162 (Dense)              (None, 7)            49          ['input_82[0][0]']               \n",
            "                                                                                                  \n",
            " dense_163 (Dense)              (None, 7)            49          ['dense_162[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_81 (TFOpLambda)   (None, 7)            0           ['dense_163[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_81 (TFOpLambd  (None, 7)           0           ['tf.ones_like_81[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_81 (TFOpL  (None, 7)           0           ['tf.math.multiply_81[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_81 (TFOpLambda)        (None, 7)            0           ['dense_163[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_81   (None, 7)           0           ['tf.convert_to_tensor_81[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_81[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_162 (TFOpL  (None,)             0           ['tf.math.squared_difference_81[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_163 (TFOpL  ()                  0           ['tf.math.reduce_mean_162[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_81 (AddLoss)          ()                   0           ['tf.math.reduce_mean_163[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/81/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0018 - val_loss: 5.6526e-06\n",
            "Epoch 2/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 3.4075e-05\n",
            "Epoch 2: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.3941e-05 - val_loss: 2.7840e-05\n",
            "Epoch 3/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.6727e-05\n",
            "Epoch 3: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7311e-05 - val_loss: 2.6775e-04\n",
            "Epoch 4/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 2.5535e-05\n",
            "Epoch 4: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.5456e-05 - val_loss: 1.2563e-05\n",
            "Epoch 5/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 2.4484e-05\n",
            "Epoch 5: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4503e-05 - val_loss: 2.1227e-05\n",
            "Epoch 6/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.7968e-05\n",
            "Epoch 6: val_loss improved from 0.00001 to 0.00000, saving model to results/81/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.7957e-05 - val_loss: 1.9089e-06\n",
            "Epoch 7/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.3722e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3679e-05 - val_loss: 4.0008e-06\n",
            "Epoch 8/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3579e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/81/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3576e-05 - val_loss: 1.6065e-06\n",
            "Epoch 9/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.2647e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2640e-05 - val_loss: 5.9919e-05\n",
            "Epoch 10/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2993e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3196e-05 - val_loss: 1.0766e-05\n",
            "Epoch 11/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.2784e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2767e-05 - val_loss: 3.1759e-05\n",
            "Epoch 12/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.2827e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2791e-05 - val_loss: 1.6959e-05\n",
            "Epoch 13/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 1.3004e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3040e-05 - val_loss: 7.6803e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.994573817697176\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_83 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_164 (Dense)              (None, 7)            49          ['input_83[0][0]']               \n",
            "                                                                                                  \n",
            " dense_165 (Dense)              (None, 7)            49          ['dense_164[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_82 (TFOpLambda)   (None, 7)            0           ['dense_165[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_82 (TFOpLambd  (None, 7)           0           ['tf.ones_like_82[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_82 (TFOpL  (None, 7)           0           ['tf.math.multiply_82[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_82 (TFOpLambda)        (None, 7)            0           ['dense_165[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_82   (None, 7)           0           ['tf.convert_to_tensor_82[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_82[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_164 (TFOpL  (None,)             0           ['tf.math.squared_difference_82[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_165 (TFOpL  ()                  0           ['tf.math.reduce_mean_164[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_82 (AddLoss)          ()                   0           ['tf.math.reduce_mean_165[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 0.0016\n",
            "Epoch 1: val_loss improved from inf to 0.00037, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0016 - val_loss: 3.6866e-04\n",
            "Epoch 2/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 4.5641e-05\n",
            "Epoch 2: val_loss improved from 0.00037 to 0.00002, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.5296e-05 - val_loss: 2.1149e-05\n",
            "Epoch 3/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 2.7146e-05\n",
            "Epoch 3: val_loss improved from 0.00002 to 0.00001, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.7069e-05 - val_loss: 6.2126e-06\n",
            "Epoch 4/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.2356e-05\n",
            "Epoch 4: val_loss did not improve from 0.00001\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.2356e-05 - val_loss: 7.1374e-04\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3323e-05\n",
            "Epoch 5: val_loss improved from 0.00001 to 0.00000, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3320e-05 - val_loss: 1.1105e-08\n",
            "Epoch 6/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.3093e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3093e-05 - val_loss: 1.8332e-04\n",
            "Epoch 7/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.3503e-05\n",
            "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3461e-05 - val_loss: 5.2391e-09\n",
            "Epoch 8/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3522e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3519e-05 - val_loss: 4.7517e-06\n",
            "Epoch 9/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.2318e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2397e-05 - val_loss: 3.9562e-04\n",
            "Epoch 10/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.2228e-05\n",
            "Epoch 10: val_loss improved from 0.00000 to 0.00000, saving model to results/82/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2220e-05 - val_loss: 1.5422e-09\n",
            "Epoch 11/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.3094e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3079e-05 - val_loss: 8.7626e-08\n",
            "Epoch 12/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.2810e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2762e-05 - val_loss: 2.7079e-06\n",
            "Epoch 13/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.3710e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3695e-05 - val_loss: 1.7529e-05\n",
            "Epoch 14/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.2038e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2019e-05 - val_loss: 9.6765e-07\n",
            "Epoch 15/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3369e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3387e-05 - val_loss: 4.3765e-05\n",
            "17649/17649 [==============================] - 24s 1ms/step\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "2644/2644 [==============================] - 4s 1ms/step\n",
            "reached auc of 0.5377534859603017\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_84 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_166 (Dense)              (None, 7)            49          ['input_84[0][0]']               \n",
            "                                                                                                  \n",
            " dense_167 (Dense)              (None, 7)            49          ['dense_166[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_83 (TFOpLambda)   (None, 7)            0           ['dense_167[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_83 (TFOpLambd  (None, 7)           0           ['tf.ones_like_83[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_83 (TFOpL  (None, 7)           0           ['tf.math.multiply_83[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_83 (TFOpLambda)        (None, 7)            0           ['dense_167[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_83   (None, 7)           0           ['tf.convert_to_tensor_83[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_83[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_166 (TFOpL  (None,)             0           ['tf.math.squared_difference_83[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_167 (TFOpL  ()                  0           ['tf.math.reduce_mean_166[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_83 (AddLoss)          ()                   0           ['tf.math.reduce_mean_167[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/83/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0018 - val_loss: 9.0332e-08\n",
            "Epoch 2/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 3.9414e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/83/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.9265e-05 - val_loss: 9.4100e-10\n",
            "Epoch 3/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 3.4486e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.4437e-05 - val_loss: 3.7520e-05\n",
            "Epoch 4/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 2.3910e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3838e-05 - val_loss: 9.7657e-06\n",
            "Epoch 5/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 2.1765e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.1688e-05 - val_loss: 9.5544e-10\n",
            "Epoch 6/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 2.0001e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.9964e-05 - val_loss: 1.6627e-08\n",
            "Epoch 7/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 2.0495e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0529e-05 - val_loss: 8.9899e-05\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.6356268851666785\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_85 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_168 (Dense)              (None, 7)            49          ['input_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_169 (Dense)              (None, 7)            49          ['dense_168[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_84 (TFOpLambda)   (None, 7)            0           ['dense_169[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_84 (TFOpLambd  (None, 7)           0           ['tf.ones_like_84[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_84 (TFOpL  (None, 7)           0           ['tf.math.multiply_84[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_84 (TFOpLambda)        (None, 7)            0           ['dense_169[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_84   (None, 7)           0           ['tf.convert_to_tensor_84[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_84[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_168 (TFOpL  (None,)             0           ['tf.math.squared_difference_84[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_169 (TFOpL  ()                  0           ['tf.math.reduce_mean_168[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_84 (AddLoss)          ()                   0           ['tf.math.reduce_mean_169[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 0.0030\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/84/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0030 - val_loss: 3.2075e-10\n",
            "Epoch 2/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 7.7968e-06\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/84/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 7.7582e-06 - val_loss: 2.3420e-10\n",
            "Epoch 3/500\n",
            "4219/4236 [============================>.] - ETA: 0s - loss: 1.0191e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0164e-05 - val_loss: 8.2684e-06\n",
            "Epoch 4/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.0365e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0316e-05 - val_loss: 2.5291e-09\n",
            "Epoch 5/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 9.9695e-06\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 9.9702e-06 - val_loss: 1.6975e-04\n",
            "Epoch 6/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.0570e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0553e-05 - val_loss: 2.9030e-07\n",
            "Epoch 7/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.0104e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0102e-05 - val_loss: 1.9392e-07\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.7021078238376716\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_86 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_170 (Dense)              (None, 7)            49          ['input_86[0][0]']               \n",
            "                                                                                                  \n",
            " dense_171 (Dense)              (None, 7)            49          ['dense_170[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_85 (TFOpLambda)   (None, 7)            0           ['dense_171[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_85 (TFOpLambd  (None, 7)           0           ['tf.ones_like_85[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_85 (TFOpL  (None, 7)           0           ['tf.math.multiply_85[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_85 (TFOpLambda)        (None, 7)            0           ['dense_171[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_85   (None, 7)           0           ['tf.convert_to_tensor_85[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_85[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_170 (TFOpL  (None,)             0           ['tf.math.squared_difference_85[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_171 (TFOpL  ()                  0           ['tf.math.reduce_mean_170[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_85 (AddLoss)          ()                   0           ['tf.math.reduce_mean_171[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/85/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0017 - val_loss: 1.1946e-07\n",
            "Epoch 2/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 4.8794e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.8834e-05 - val_loss: 8.0485e-05\n",
            "Epoch 3/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 3.7917e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/85/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 3.7899e-05 - val_loss: 1.8272e-08\n",
            "Epoch 4/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 3.7069e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 3.7540e-05 - val_loss: 1.0074e-04\n",
            "Epoch 5/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 2.7853e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.7656e-05 - val_loss: 1.1401e-07\n",
            "Epoch 6/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.5066e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5066e-05 - val_loss: 3.4229e-07\n",
            "Epoch 7/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 2.0793e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.0759e-05 - val_loss: 1.2865e-06\n",
            "Epoch 8/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.5986e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5947e-05 - val_loss: 2.0075e-05\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999955893719669\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_87 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_172 (Dense)              (None, 7)            49          ['input_87[0][0]']               \n",
            "                                                                                                  \n",
            " dense_173 (Dense)              (None, 7)            49          ['dense_172[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_86 (TFOpLambda)   (None, 7)            0           ['dense_173[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_86 (TFOpLambd  (None, 7)           0           ['tf.ones_like_86[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_86 (TFOpL  (None, 7)           0           ['tf.math.multiply_86[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_86 (TFOpLambda)        (None, 7)            0           ['dense_173[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_86   (None, 7)           0           ['tf.convert_to_tensor_86[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_86[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_172 (TFOpL  (None,)             0           ['tf.math.squared_difference_86[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_173 (TFOpL  ()                  0           ['tf.math.reduce_mean_172[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_86 (AddLoss)          ()                   0           ['tf.math.reduce_mean_173[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0019 - val_loss: 4.9018e-06\n",
            "Epoch 2/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 4.3391e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 4.3307e-05 - val_loss: 2.9556e-06\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.6360e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.6395e-05 - val_loss: 1.2713e-05\n",
            "Epoch 4/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 2.0155e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.0070e-05 - val_loss: 2.8670e-06\n",
            "Epoch 5/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.6380e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6686e-05 - val_loss: 6.1712e-05\n",
            "Epoch 6/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.4888e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4894e-05 - val_loss: 7.0986e-06\n",
            "Epoch 7/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.5240e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.5237e-05 - val_loss: 1.5880e-05\n",
            "Epoch 8/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.4749e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4746e-05 - val_loss: 7.1495e-07\n",
            "Epoch 9/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.4748e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4738e-05 - val_loss: 1.2589e-07\n",
            "Epoch 10/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.5089e-05\n",
            "Epoch 10: val_loss improved from 0.00000 to 0.00000, saving model to results/86/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5015e-05 - val_loss: 6.6455e-08\n",
            "Epoch 11/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.5004e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4943e-05 - val_loss: 7.8073e-07\n",
            "Epoch 12/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.4754e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4750e-05 - val_loss: 8.4155e-08\n",
            "Epoch 13/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.4815e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4767e-05 - val_loss: 1.3155e-07\n",
            "Epoch 14/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.4999e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5339e-05 - val_loss: 7.3029e-05\n",
            "Epoch 15/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 1.5015e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4924e-05 - val_loss: 1.6619e-07\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9699814994723792\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_88 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_174 (Dense)              (None, 7)            49          ['input_88[0][0]']               \n",
            "                                                                                                  \n",
            " dense_175 (Dense)              (None, 7)            49          ['dense_174[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_87 (TFOpLambda)   (None, 7)            0           ['dense_175[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_87 (TFOpLambd  (None, 7)           0           ['tf.ones_like_87[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_87 (TFOpL  (None, 7)           0           ['tf.math.multiply_87[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_87 (TFOpLambda)        (None, 7)            0           ['dense_175[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_87   (None, 7)           0           ['tf.convert_to_tensor_87[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_87[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_174 (TFOpL  (None,)             0           ['tf.math.squared_difference_87[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_175 (TFOpL  ()                  0           ['tf.math.reduce_mean_174[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_87 (AddLoss)          ()                   0           ['tf.math.reduce_mean_175[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 0.0019\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/87/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0019 - val_loss: 1.0484e-05\n",
            "Epoch 2/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 4.5657e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00000, saving model to results/87/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 4.5488e-05 - val_loss: 5.6168e-07\n",
            "Epoch 3/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 3.5401e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.5429e-05 - val_loss: 1.6187e-04\n",
            "Epoch 4/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 2.4882e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4796e-05 - val_loss: 4.4377e-06\n",
            "Epoch 5/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.8062e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/87/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7979e-05 - val_loss: 8.8357e-08\n",
            "Epoch 6/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.3597e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3569e-05 - val_loss: 8.7909e-06\n",
            "Epoch 7/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2377e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2322e-05 - val_loss: 4.3080e-07\n",
            "Epoch 8/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.2597e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2643e-05 - val_loss: 1.2736e-05\n",
            "Epoch 9/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.2192e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/87/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2162e-05 - val_loss: 5.9429e-08\n",
            "Epoch 10/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.2717e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.2716e-05 - val_loss: 2.8393e-07\n",
            "Epoch 11/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.2253e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2438e-05 - val_loss: 3.3170e-04\n",
            "Epoch 12/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.2687e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2646e-05 - val_loss: 6.8373e-08\n",
            "Epoch 13/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.2141e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2141e-05 - val_loss: 1.1617e-06\n",
            "Epoch 14/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.2258e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2367e-05 - val_loss: 1.0914e-04\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 4s 1ms/step\n",
            "reached auc of 0.857187723452594\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_89 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_176 (Dense)              (None, 7)            49          ['input_89[0][0]']               \n",
            "                                                                                                  \n",
            " dense_177 (Dense)              (None, 7)            49          ['dense_176[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_88 (TFOpLambda)   (None, 7)            0           ['dense_177[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_88 (TFOpLambd  (None, 7)           0           ['tf.ones_like_88[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_88 (TFOpL  (None, 7)           0           ['tf.math.multiply_88[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_88 (TFOpLambda)        (None, 7)            0           ['dense_177[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_88   (None, 7)           0           ['tf.convert_to_tensor_88[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_88[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_176 (TFOpL  (None,)             0           ['tf.math.squared_difference_88[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_177 (TFOpL  ()                  0           ['tf.math.reduce_mean_176[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_88 (AddLoss)          ()                   0           ['tf.math.reduce_mean_177[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 0.0026\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to results/88/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0026 - val_loss: 3.9093e-05\n",
            "Epoch 2/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 3.0962e-05\n",
            "Epoch 2: val_loss improved from 0.00004 to 0.00000, saving model to results/88/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.0896e-05 - val_loss: 7.0288e-08\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.7485e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.7362e-05 - val_loss: 7.4758e-08\n",
            "Epoch 4/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 2.2787e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3144e-05 - val_loss: 7.4172e-05\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.2812e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2808e-05 - val_loss: 9.8026e-07\n",
            "Epoch 6/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.9492e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9488e-05 - val_loss: 1.9858e-06\n",
            "Epoch 7/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.5324e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5318e-05 - val_loss: 2.9096e-06\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 1.0\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_90 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_178 (Dense)              (None, 7)            49          ['input_90[0][0]']               \n",
            "                                                                                                  \n",
            " dense_179 (Dense)              (None, 7)            49          ['dense_178[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_89 (TFOpLambda)   (None, 7)            0           ['dense_179[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_89 (TFOpLambd  (None, 7)           0           ['tf.ones_like_89[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_89 (TFOpL  (None, 7)           0           ['tf.math.multiply_89[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_89 (TFOpLambda)        (None, 7)            0           ['dense_179[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_89   (None, 7)           0           ['tf.convert_to_tensor_89[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_89[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_178 (TFOpL  (None,)             0           ['tf.math.squared_difference_89[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_179 (TFOpL  ()                  0           ['tf.math.reduce_mean_178[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_89 (AddLoss)          ()                   0           ['tf.math.reduce_mean_179[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00033, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0021 - val_loss: 3.2676e-04\n",
            "Epoch 2/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 3.2096e-05\n",
            "Epoch 2: val_loss improved from 0.00033 to 0.00001, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.1984e-05 - val_loss: 5.2956e-06\n",
            "Epoch 3/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 2.7340e-05\n",
            "Epoch 3: val_loss improved from 0.00001 to 0.00000, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 2.7220e-05 - val_loss: 1.4595e-06\n",
            "Epoch 4/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 1.9661e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9715e-05 - val_loss: 3.5797e-06\n",
            "Epoch 5/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 1.5825e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.5799e-05 - val_loss: 4.2640e-07\n",
            "Epoch 6/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.5080e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4979e-05 - val_loss: 2.1382e-06\n",
            "Epoch 7/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.2977e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2957e-05 - val_loss: 2.5102e-06\n",
            "Epoch 8/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.5211e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5107e-05 - val_loss: 3.6092e-07\n",
            "Epoch 9/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.3259e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3256e-05 - val_loss: 9.6202e-07\n",
            "Epoch 10/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.3983e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.3983e-05 - val_loss: 6.6661e-07\n",
            "Epoch 11/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.4525e-05\n",
            "Epoch 11: val_loss improved from 0.00000 to 0.00000, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.4463e-05 - val_loss: 3.5584e-07\n",
            "Epoch 12/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.3576e-05\n",
            "Epoch 12: val_loss improved from 0.00000 to 0.00000, saving model to results/89/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3518e-05 - val_loss: 3.5181e-07\n",
            "Epoch 13/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 1.3415e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3680e-05 - val_loss: 1.1251e-05\n",
            "Epoch 14/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.3999e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3994e-05 - val_loss: 4.7792e-06\n",
            "Epoch 15/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.4861e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4828e-05 - val_loss: 8.9366e-07\n",
            "Epoch 16/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.3039e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3036e-05 - val_loss: 8.9807e-07\n",
            "Epoch 17/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.4083e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4046e-05 - val_loss: 1.8339e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8409508757169974\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_91 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_180 (Dense)              (None, 7)            49          ['input_91[0][0]']               \n",
            "                                                                                                  \n",
            " dense_181 (Dense)              (None, 7)            49          ['dense_180[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_90 (TFOpLambda)   (None, 7)            0           ['dense_181[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_90 (TFOpLambd  (None, 7)           0           ['tf.ones_like_90[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_90 (TFOpL  (None, 7)           0           ['tf.math.multiply_90[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_90 (TFOpLambda)        (None, 7)            0           ['dense_181[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_90   (None, 7)           0           ['tf.convert_to_tensor_90[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_90[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_180 (TFOpL  (None,)             0           ['tf.math.squared_difference_90[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_181 (TFOpL  ()                  0           ['tf.math.reduce_mean_180[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_90 (AddLoss)          ()                   0           ['tf.math.reduce_mean_181[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0017 - val_loss: 2.5655e-07\n",
            "Epoch 2/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 4.3131e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.2979e-05 - val_loss: 5.3233e-07\n",
            "Epoch 3/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 2.4021e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4004e-05 - val_loss: 2.3966e-07\n",
            "Epoch 4/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.5510e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5424e-05 - val_loss: 1.5689e-06\n",
            "Epoch 5/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.5941e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5899e-05 - val_loss: 1.6568e-06\n",
            "Epoch 6/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.5460e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5389e-05 - val_loss: 2.9779e-07\n",
            "Epoch 7/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 1.5231e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5162e-05 - val_loss: 1.5574e-06\n",
            "Epoch 8/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.4995e-05\n",
            "Epoch 8: val_loss improved from 0.00000 to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4960e-05 - val_loss: 1.5861e-07\n",
            "Epoch 9/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.6364e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6360e-05 - val_loss: 1.4516e-07\n",
            "Epoch 10/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.3830e-05\n",
            "Epoch 10: val_loss improved from 0.00000 to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3788e-05 - val_loss: 1.3930e-07\n",
            "Epoch 11/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.6717e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6670e-05 - val_loss: 1.6401e-07\n",
            "Epoch 12/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.3565e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3495e-05 - val_loss: 1.6028e-06\n",
            "Epoch 13/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.4618e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4615e-05 - val_loss: 1.7681e-07\n",
            "Epoch 14/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.4371e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4341e-05 - val_loss: 4.4617e-07\n",
            "Epoch 15/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 1.5600e-05\n",
            "Epoch 15: val_loss improved from 0.00000 to 0.00000, saving model to results/90/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5578e-05 - val_loss: 9.8608e-08\n",
            "Epoch 16/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.5275e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5225e-05 - val_loss: 5.2304e-07\n",
            "Epoch 17/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.3861e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3831e-05 - val_loss: 1.7433e-06\n",
            "Epoch 18/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.4269e-05\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4252e-05 - val_loss: 3.2848e-07\n",
            "Epoch 19/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.5803e-05\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5786e-05 - val_loss: 4.8303e-06\n",
            "Epoch 20/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 1.3695e-05\n",
            "Epoch 20: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3695e-05 - val_loss: 1.1412e-05\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999999952896023\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_92 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_182 (Dense)              (None, 7)            49          ['input_92[0][0]']               \n",
            "                                                                                                  \n",
            " dense_183 (Dense)              (None, 7)            49          ['dense_182[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_91 (TFOpLambda)   (None, 7)            0           ['dense_183[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_91 (TFOpLambd  (None, 7)           0           ['tf.ones_like_91[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_91 (TFOpL  (None, 7)           0           ['tf.math.multiply_91[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_91 (TFOpLambda)        (None, 7)            0           ['dense_183[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_91   (None, 7)           0           ['tf.convert_to_tensor_91[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_91[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_182 (TFOpL  (None,)             0           ['tf.math.squared_difference_91[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_183 (TFOpL  ()                  0           ['tf.math.reduce_mean_182[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_91 (AddLoss)          ()                   0           ['tf.math.reduce_mean_183[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 0.0017\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to results/91/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0017 - val_loss: 3.5659e-05\n",
            "Epoch 2/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 5.4291e-05\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 5.4591e-05 - val_loss: 8.5151e-05\n",
            "Epoch 3/500\n",
            "4234/4236 [============================>.] - ETA: 0s - loss: 3.6596e-05\n",
            "Epoch 3: val_loss improved from 0.00004 to 0.00000, saving model to results/91/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.6583e-05 - val_loss: 3.7832e-06\n",
            "Epoch 4/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 2.3858e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/91/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3741e-05 - val_loss: 1.9902e-06\n",
            "Epoch 5/500\n",
            "4209/4236 [============================>.] - ETA: 0s - loss: 1.6863e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6821e-05 - val_loss: 7.4381e-06\n",
            "Epoch 6/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.6910e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6868e-05 - val_loss: 7.8125e-06\n",
            "Epoch 7/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.6967e-05\n",
            "Epoch 7: val_loss improved from 0.00000 to 0.00000, saving model to results/91/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6924e-05 - val_loss: 5.9420e-07\n",
            "Epoch 8/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.5353e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5305e-05 - val_loss: 1.1945e-06\n",
            "Epoch 9/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.5700e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5691e-05 - val_loss: 2.3371e-05\n",
            "Epoch 10/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.5973e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.6355e-05 - val_loss: 8.4797e-06\n",
            "Epoch 11/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.5059e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5031e-05 - val_loss: 5.8646e-06\n",
            "Epoch 12/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.5876e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 1.6017e-05 - val_loss: 5.9193e-06\n",
            "17649/17649 [==============================] - 23s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9913376320453308\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_93 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_184 (Dense)              (None, 7)            49          ['input_93[0][0]']               \n",
            "                                                                                                  \n",
            " dense_185 (Dense)              (None, 7)            49          ['dense_184[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_92 (TFOpLambda)   (None, 7)            0           ['dense_185[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_92 (TFOpLambd  (None, 7)           0           ['tf.ones_like_92[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_92 (TFOpL  (None, 7)           0           ['tf.math.multiply_92[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_92 (TFOpLambda)        (None, 7)            0           ['dense_185[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_92   (None, 7)           0           ['tf.convert_to_tensor_92[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_92[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_184 (TFOpL  (None,)             0           ['tf.math.squared_difference_92[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_185 (TFOpL  ()                  0           ['tf.math.reduce_mean_184[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_92 (AddLoss)          ()                   0           ['tf.math.reduce_mean_185[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 0.0018\n",
            "Epoch 1: val_loss improved from inf to 0.00001, saving model to results/92/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0018 - val_loss: 7.6811e-06\n",
            "Epoch 2/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 3.6941e-05\n",
            "Epoch 2: val_loss improved from 0.00001 to 0.00000, saving model to results/92/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.6714e-05 - val_loss: 1.3075e-07\n",
            "Epoch 3/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 2.9115e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.8971e-05 - val_loss: 1.4430e-04\n",
            "Epoch 4/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.9943e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9869e-05 - val_loss: 1.3928e-06\n",
            "Epoch 5/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.8025e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/92/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7958e-05 - val_loss: 1.7436e-08\n",
            "Epoch 6/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.2613e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2579e-05 - val_loss: 8.5583e-05\n",
            "Epoch 7/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 1.0070e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0099e-05 - val_loss: 4.0946e-05\n",
            "Epoch 8/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.0158e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0158e-05 - val_loss: 2.5908e-06\n",
            "Epoch 9/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.0197e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 7s 2ms/step - loss: 1.0145e-05 - val_loss: 1.8937e-06\n",
            "Epoch 10/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.0416e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0414e-05 - val_loss: 7.1194e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5853004937117647\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_94 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_186 (Dense)              (None, 7)            49          ['input_94[0][0]']               \n",
            "                                                                                                  \n",
            " dense_187 (Dense)              (None, 7)            49          ['dense_186[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_93 (TFOpLambda)   (None, 7)            0           ['dense_187[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_93 (TFOpLambd  (None, 7)           0           ['tf.ones_like_93[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_93 (TFOpL  (None, 7)           0           ['tf.math.multiply_93[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_93 (TFOpLambda)        (None, 7)            0           ['dense_187[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_93   (None, 7)           0           ['tf.convert_to_tensor_93[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_93[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_186 (TFOpL  (None,)             0           ['tf.math.squared_difference_93[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_187 (TFOpL  ()                  0           ['tf.math.reduce_mean_186[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_93 (AddLoss)          ()                   0           ['tf.math.reduce_mean_187[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 0.0016\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0016 - val_loss: 4.9417e-06\n",
            "Epoch 2/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 4.5689e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 4.5763e-05 - val_loss: 4.8586e-05\n",
            "Epoch 3/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 2.9576e-05\n",
            "Epoch 3: val_loss improved from 0.00000 to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.9476e-05 - val_loss: 2.0308e-06\n",
            "Epoch 4/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.9032e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.8989e-05 - val_loss: 5.7941e-05\n",
            "Epoch 5/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.5535e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5468e-05 - val_loss: 1.7923e-06\n",
            "Epoch 6/500\n",
            "4214/4236 [============================>.] - ETA: 0s - loss: 1.3622e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3554e-05 - val_loss: 4.3403e-07\n",
            "Epoch 7/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.3619e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 7s 2ms/step - loss: 1.3658e-05 - val_loss: 1.0547e-05\n",
            "Epoch 8/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.4057e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4019e-05 - val_loss: 2.6478e-06\n",
            "Epoch 9/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.4000e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3924e-05 - val_loss: 6.2873e-07\n",
            "Epoch 10/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.4264e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4162e-05 - val_loss: 6.2733e-07\n",
            "Epoch 11/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.3604e-05\n",
            "Epoch 11: val_loss improved from 0.00000 to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3536e-05 - val_loss: 7.7676e-08\n",
            "Epoch 12/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.4418e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4350e-05 - val_loss: 1.7751e-07\n",
            "Epoch 13/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.4133e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4075e-05 - val_loss: 2.7848e-07\n",
            "Epoch 14/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.4145e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 7s 2ms/step - loss: 1.4129e-05 - val_loss: 5.9026e-07\n",
            "Epoch 15/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.3224e-05\n",
            "Epoch 15: val_loss improved from 0.00000 to 0.00000, saving model to results/93/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3215e-05 - val_loss: 3.3691e-08\n",
            "Epoch 16/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.3430e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3467e-05 - val_loss: 1.1137e-05\n",
            "Epoch 17/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.3626e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3626e-05 - val_loss: 2.6931e-07\n",
            "Epoch 18/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.3283e-05\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3284e-05 - val_loss: 6.2490e-06\n",
            "Epoch 19/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 1.4555e-05\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.4555e-05 - val_loss: 1.9505e-07\n",
            "Epoch 20/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.3960e-05\n",
            "Epoch 20: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3917e-05 - val_loss: 6.0397e-08\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.8297950406591235\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_95 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_188 (Dense)              (None, 7)            49          ['input_95[0][0]']               \n",
            "                                                                                                  \n",
            " dense_189 (Dense)              (None, 7)            49          ['dense_188[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_94 (TFOpLambda)   (None, 7)            0           ['dense_189[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_94 (TFOpLambd  (None, 7)           0           ['tf.ones_like_94[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_94 (TFOpL  (None, 7)           0           ['tf.math.multiply_94[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_94 (TFOpLambda)        (None, 7)            0           ['dense_189[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_94   (None, 7)           0           ['tf.convert_to_tensor_94[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_94[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_188 (TFOpL  (None,)             0           ['tf.math.squared_difference_94[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_189 (TFOpL  ()                  0           ['tf.math.reduce_mean_188[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_94 (AddLoss)          ()                   0           ['tf.math.reduce_mean_189[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4198/4236 [============================>.] - ETA: 0s - loss: 0.0021\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/94/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0021 - val_loss: 1.9513e-07\n",
            "Epoch 2/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 5.2510e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/94/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 5.2214e-05 - val_loss: 1.0859e-07\n",
            "Epoch 3/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 3.1860e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 3.1831e-05 - val_loss: 4.0113e-07\n",
            "Epoch 4/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.8199e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.8427e-05 - val_loss: 1.9663e-04\n",
            "Epoch 5/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 1.8186e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/94/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8040e-05 - val_loss: 1.7334e-10\n",
            "Epoch 6/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 1.5495e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5403e-05 - val_loss: 2.0374e-08\n",
            "Epoch 7/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.5410e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.5323e-05 - val_loss: 4.4665e-10\n",
            "Epoch 8/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 1.6102e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5969e-05 - val_loss: 9.4271e-10\n",
            "Epoch 9/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.5491e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.5458e-05 - val_loss: 3.1230e-08\n",
            "Epoch 10/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 1.7687e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.7705e-05 - val_loss: 2.8194e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 1.0\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_96 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_190 (Dense)              (None, 7)            49          ['input_96[0][0]']               \n",
            "                                                                                                  \n",
            " dense_191 (Dense)              (None, 7)            49          ['dense_190[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_95 (TFOpLambda)   (None, 7)            0           ['dense_191[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_95 (TFOpLambd  (None, 7)           0           ['tf.ones_like_95[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_95 (TFOpL  (None, 7)           0           ['tf.math.multiply_95[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_95 (TFOpLambda)        (None, 7)            0           ['dense_191[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_95   (None, 7)           0           ['tf.convert_to_tensor_95[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_95[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_190 (TFOpL  (None,)             0           ['tf.math.squared_difference_95[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_191 (TFOpL  ()                  0           ['tf.math.reduce_mean_190[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_95 (AddLoss)          ()                   0           ['tf.math.reduce_mean_191[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 0.0022\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0022 - val_loss: 3.7646e-06\n",
            "Epoch 2/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 2.5441e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5296e-05 - val_loss: 1.8366e-06\n",
            "Epoch 3/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 2.5792e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.5678e-05 - val_loss: 3.7067e-06\n",
            "Epoch 4/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 2.5573e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5625e-05 - val_loss: 2.6091e-06\n",
            "Epoch 5/500\n",
            "4208/4236 [============================>.] - ETA: 0s - loss: 2.5140e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4993e-05 - val_loss: 3.6923e-06\n",
            "Epoch 6/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 2.4890e-05\n",
            "Epoch 6: val_loss improved from 0.00000 to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4813e-05 - val_loss: 1.0002e-06\n",
            "Epoch 7/500\n",
            "4236/4236 [==============================] - ETA: 0s - loss: 2.4897e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4897e-05 - val_loss: 4.0153e-05\n",
            "Epoch 8/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 2.4665e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4651e-05 - val_loss: 4.8488e-05\n",
            "Epoch 9/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.4922e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4871e-05 - val_loss: 6.3796e-06\n",
            "Epoch 10/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 2.4654e-05\n",
            "Epoch 10: val_loss improved from 0.00000 to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4573e-05 - val_loss: 3.9796e-07\n",
            "Epoch 11/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 2.4395e-05\n",
            "Epoch 11: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4214e-05 - val_loss: 4.4745e-07\n",
            "Epoch 12/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 2.3945e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.3809e-05 - val_loss: 2.5721e-06\n",
            "Epoch 13/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 2.3528e-05\n",
            "Epoch 13: val_loss improved from 0.00000 to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3506e-05 - val_loss: 3.6004e-07\n",
            "Epoch 14/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.9740e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9673e-05 - val_loss: 4.0502e-07\n",
            "Epoch 15/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 1.3928e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3839e-05 - val_loss: 4.9553e-07\n",
            "Epoch 16/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.3339e-05\n",
            "Epoch 16: val_loss improved from 0.00000 to 0.00000, saving model to results/95/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3250e-05 - val_loss: 3.4768e-07\n",
            "Epoch 17/500\n",
            "4207/4236 [============================>.] - ETA: 0s - loss: 1.2242e-05\n",
            "Epoch 17: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2248e-05 - val_loss: 4.8466e-06\n",
            "Epoch 18/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 1.2591e-05\n",
            "Epoch 18: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2538e-05 - val_loss: 6.3195e-06\n",
            "Epoch 19/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 1.2215e-05\n",
            "Epoch 19: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2134e-05 - val_loss: 3.5163e-06\n",
            "Epoch 20/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.2716e-05\n",
            "Epoch 20: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2640e-05 - val_loss: 1.9916e-06\n",
            "Epoch 21/500\n",
            "4201/4236 [============================>.] - ETA: 0s - loss: 1.6573e-05\n",
            "Epoch 21: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.6672e-05 - val_loss: 3.6163e-06\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.9999656192569588\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_97 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_192 (Dense)              (None, 7)            49          ['input_97[0][0]']               \n",
            "                                                                                                  \n",
            " dense_193 (Dense)              (None, 7)            49          ['dense_192[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_96 (TFOpLambda)   (None, 7)            0           ['dense_193[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_96 (TFOpLambd  (None, 7)           0           ['tf.ones_like_96[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_96 (TFOpL  (None, 7)           0           ['tf.math.multiply_96[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_96 (TFOpLambda)        (None, 7)            0           ['dense_193[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_96   (None, 7)           0           ['tf.convert_to_tensor_96[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_96[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_192 (TFOpL  (None,)             0           ['tf.math.squared_difference_96[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_193 (TFOpL  ()                  0           ['tf.math.reduce_mean_192[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_96 (AddLoss)          ()                   0           ['tf.math.reduce_mean_193[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 0.0023\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to results/96/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 0.0023 - val_loss: 3.9746e-05\n",
            "Epoch 2/500\n",
            "4230/4236 [============================>.] - ETA: 0s - loss: 1.8989e-05\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8970e-05 - val_loss: 1.0391e-04\n",
            "Epoch 3/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.0863e-05\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0770e-05 - val_loss: 4.0371e-05\n",
            "Epoch 4/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 1.9153e-05\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9163e-05 - val_loss: 1.1780e-04\n",
            "Epoch 5/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.0478e-05\n",
            "Epoch 5: val_loss improved from 0.00004 to 0.00004, saving model to results/96/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0473e-05 - val_loss: 3.8892e-05\n",
            "Epoch 6/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 2.0112e-05\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9993e-05 - val_loss: 4.0786e-05\n",
            "Epoch 7/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 2.1176e-05\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.1144e-05 - val_loss: 7.3175e-05\n",
            "Epoch 8/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 1.9031e-05\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.8946e-05 - val_loss: 5.3612e-05\n",
            "Epoch 9/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 2.1810e-05\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00004, saving model to results/96/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.1789e-05 - val_loss: 3.5458e-05\n",
            "Epoch 10/500\n",
            "4202/4236 [============================>.] - ETA: 0s - loss: 2.1543e-05\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.1376e-05 - val_loss: 3.5569e-05\n",
            "Epoch 11/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 2.1556e-05\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.1454e-05 - val_loss: 3.9718e-05\n",
            "Epoch 12/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 1.9770e-05\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9725e-05 - val_loss: 4.9583e-05\n",
            "Epoch 13/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.8444e-05\n",
            "Epoch 13: val_loss improved from 0.00004 to 0.00004, saving model to results/96/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8401e-05 - val_loss: 3.5038e-05\n",
            "Epoch 14/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.4678e-05\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.4644e-05 - val_loss: 2.9363e-04\n",
            "Epoch 15/500\n",
            "4227/4236 [============================>.] - ETA: 0s - loss: 1.0352e-05\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0330e-05 - val_loss: 3.9604e-05\n",
            "Epoch 16/500\n",
            "4206/4236 [============================>.] - ETA: 0s - loss: 1.0850e-05\n",
            "Epoch 16: val_loss improved from 0.00004 to 0.00003, saving model to results/96/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0809e-05 - val_loss: 3.2713e-05\n",
            "Epoch 17/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.0911e-05\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0832e-05 - val_loss: 3.9883e-05\n",
            "Epoch 18/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 1.0371e-05\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0358e-05 - val_loss: 3.9647e-05\n",
            "Epoch 19/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.0476e-05\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.0424e-05 - val_loss: 3.9847e-05\n",
            "Epoch 20/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.0331e-05\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0282e-05 - val_loss: 3.9773e-05\n",
            "Epoch 21/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.0545e-05\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.0513e-05 - val_loss: 3.9684e-05\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.668580230872974\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_98 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_194 (Dense)              (None, 7)            49          ['input_98[0][0]']               \n",
            "                                                                                                  \n",
            " dense_195 (Dense)              (None, 7)            49          ['dense_194[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_97 (TFOpLambda)   (None, 7)            0           ['dense_195[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_97 (TFOpLambd  (None, 7)           0           ['tf.ones_like_97[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_97 (TFOpL  (None, 7)           0           ['tf.math.multiply_97[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_97 (TFOpLambda)        (None, 7)            0           ['dense_195[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_97   (None, 7)           0           ['tf.convert_to_tensor_97[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_97[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_194 (TFOpL  (None,)             0           ['tf.math.squared_difference_97[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_195 (TFOpL  ()                  0           ['tf.math.reduce_mean_194[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_97 (AddLoss)          ()                   0           ['tf.math.reduce_mean_195[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4211/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00081, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 0.0020 - val_loss: 8.1021e-04\n",
            "Epoch 2/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 3.0231e-05\n",
            "Epoch 2: val_loss improved from 0.00081 to 0.00001, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 3.0176e-05 - val_loss: 5.8414e-06\n",
            "Epoch 3/500\n",
            "4200/4236 [============================>.] - ETA: 0s - loss: 2.5504e-05\n",
            "Epoch 3: val_loss improved from 0.00001 to 0.00000, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.5305e-05 - val_loss: 3.7449e-06\n",
            "Epoch 4/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 2.4738e-05\n",
            "Epoch 4: val_loss improved from 0.00000 to 0.00000, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.4616e-05 - val_loss: 3.3548e-08\n",
            "Epoch 5/500\n",
            "4221/4236 [============================>.] - ETA: 0s - loss: 2.3123e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3042e-05 - val_loss: 6.5208e-07\n",
            "Epoch 6/500\n",
            "4212/4236 [============================>.] - ETA: 0s - loss: 2.3953e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.4170e-05 - val_loss: 3.0467e-05\n",
            "Epoch 7/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 2.0721e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.0716e-05 - val_loss: 4.5208e-07\n",
            "Epoch 8/500\n",
            "4216/4236 [============================>.] - ETA: 0s - loss: 1.3755e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3690e-05 - val_loss: 8.0752e-07\n",
            "Epoch 9/500\n",
            "4229/4236 [============================>.] - ETA: 0s - loss: 1.3923e-05\n",
            "Epoch 9: val_loss improved from 0.00000 to 0.00000, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.3900e-05 - val_loss: 2.1636e-09\n",
            "Epoch 10/500\n",
            "4215/4236 [============================>.] - ETA: 0s - loss: 1.1053e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1117e-05 - val_loss: 1.6041e-07\n",
            "Epoch 11/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.1977e-05\n",
            "Epoch 11: val_loss improved from 0.00000 to 0.00000, saving model to results/97/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1943e-05 - val_loss: 6.5333e-10\n",
            "Epoch 12/500\n",
            "4218/4236 [============================>.] - ETA: 0s - loss: 1.1797e-05\n",
            "Epoch 12: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1747e-05 - val_loss: 6.5790e-07\n",
            "Epoch 13/500\n",
            "4205/4236 [============================>.] - ETA: 0s - loss: 1.2603e-05\n",
            "Epoch 13: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2511e-05 - val_loss: 1.3386e-09\n",
            "Epoch 14/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 1.1968e-05\n",
            "Epoch 14: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1934e-05 - val_loss: 2.1809e-07\n",
            "Epoch 15/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 1.2082e-05\n",
            "Epoch 15: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2072e-05 - val_loss: 2.8002e-08\n",
            "Epoch 16/500\n",
            "4204/4236 [============================>.] - ETA: 0s - loss: 1.1831e-05\n",
            "Epoch 16: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.1763e-05 - val_loss: 1.7605e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.6934944623482386\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_99 (InputLayer)          [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_196 (Dense)              (None, 7)            49          ['input_99[0][0]']               \n",
            "                                                                                                  \n",
            " dense_197 (Dense)              (None, 7)            49          ['dense_196[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_98 (TFOpLambda)   (None, 7)            0           ['dense_197[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_98 (TFOpLambd  (None, 7)           0           ['tf.ones_like_98[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_98 (TFOpL  (None, 7)           0           ['tf.math.multiply_98[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_98 (TFOpLambda)        (None, 7)            0           ['dense_197[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_98   (None, 7)           0           ['tf.convert_to_tensor_98[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_98[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_196 (TFOpL  (None,)             0           ['tf.math.squared_difference_98[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_197 (TFOpL  ()                  0           ['tf.math.reduce_mean_196[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_98 (AddLoss)          ()                   0           ['tf.math.reduce_mean_197[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4235/4236 [============================>.] - ETA: 0s - loss: 0.0020\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/98/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0020 - val_loss: 6.3635e-10\n",
            "Epoch 2/500\n",
            "4217/4236 [============================>.] - ETA: 0s - loss: 2.5758e-05\n",
            "Epoch 2: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.5656e-05 - val_loss: 3.0773e-04\n",
            "Epoch 3/500\n",
            "4220/4236 [============================>.] - ETA: 0s - loss: 2.0944e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.0865e-05 - val_loss: 2.5025e-09\n",
            "Epoch 4/500\n",
            "4203/4236 [============================>.] - ETA: 0s - loss: 2.0016e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.9860e-05 - val_loss: 1.1877e-07\n",
            "Epoch 5/500\n",
            "4232/4236 [============================>.] - ETA: 0s - loss: 2.1815e-05\n",
            "Epoch 5: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.1959e-05 - val_loss: 4.1109e-05\n",
            "Epoch 6/500\n",
            "4222/4236 [============================>.] - ETA: 0s - loss: 1.9253e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.9191e-05 - val_loss: 2.0555e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.6453102142681758\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"oneoff\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_100 (InputLayer)         [(None, 7)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_198 (Dense)              (None, 7)            49          ['input_100[0][0]']              \n",
            "                                                                                                  \n",
            " dense_199 (Dense)              (None, 7)            49          ['dense_198[0][0]']              \n",
            "                                                                                                  \n",
            " tf.ones_like_99 (TFOpLambda)   (None, 7)            0           ['dense_199[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_99 (TFOpLambd  (None, 7)           0           ['tf.ones_like_99[0][0]']        \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor_99 (TFOpL  (None, 7)           0           ['tf.math.multiply_99[0][0]']    \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast_99 (TFOpLambda)        (None, 7)            0           ['dense_199[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.squared_difference_99   (None, 7)           0           ['tf.convert_to_tensor_99[0][0]',\n",
            " (TFOpLambda)                                                     'tf.cast_99[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_198 (TFOpL  (None,)             0           ['tf.math.squared_difference_99[0\n",
            " ambda)                                                          ][0]']                           \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_199 (TFOpL  ()                  0           ['tf.math.reduce_mean_198[0][0]']\n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " add_loss_99 (AddLoss)          ()                   0           ['tf.math.reduce_mean_199[0][0]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 98\n",
            "Trainable params: 98\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "4225/4236 [============================>.] - ETA: 0s - loss: 0.0023\n",
            "Epoch 1: val_loss improved from inf to 0.00000, saving model to results/99/model.tf\n",
            "4236/4236 [==============================] - 10s 2ms/step - loss: 0.0023 - val_loss: 9.6147e-08\n",
            "Epoch 2/500\n",
            "4228/4236 [============================>.] - ETA: 0s - loss: 2.3938e-05\n",
            "Epoch 2: val_loss improved from 0.00000 to 0.00000, saving model to results/99/model.tf\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 2.3893e-05 - val_loss: 1.6788e-08\n",
            "Epoch 3/500\n",
            "4231/4236 [============================>.] - ETA: 0s - loss: 2.2275e-05\n",
            "Epoch 3: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 2.2249e-05 - val_loss: 4.2702e-07\n",
            "Epoch 4/500\n",
            "4210/4236 [============================>.] - ETA: 0s - loss: 1.8160e-05\n",
            "Epoch 4: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.8589e-05 - val_loss: 1.3370e-04\n",
            "Epoch 5/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.3613e-05\n",
            "Epoch 5: val_loss improved from 0.00000 to 0.00000, saving model to results/99/model.tf\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.3540e-05 - val_loss: 1.4399e-08\n",
            "Epoch 6/500\n",
            "4224/4236 [============================>.] - ETA: 0s - loss: 1.2222e-05\n",
            "Epoch 6: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 9s 2ms/step - loss: 1.2188e-05 - val_loss: 1.2852e-07\n",
            "Epoch 7/500\n",
            "4223/4236 [============================>.] - ETA: 0s - loss: 1.2262e-05\n",
            "Epoch 7: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2225e-05 - val_loss: 1.0075e-06\n",
            "Epoch 8/500\n",
            "4226/4236 [============================>.] - ETA: 0s - loss: 1.2599e-05\n",
            "Epoch 8: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.2742e-05 - val_loss: 6.1536e-06\n",
            "Epoch 9/500\n",
            "4213/4236 [============================>.] - ETA: 0s - loss: 1.1733e-05\n",
            "Epoch 9: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1960e-05 - val_loss: 4.3761e-05\n",
            "Epoch 10/500\n",
            "4233/4236 [============================>.] - ETA: 0s - loss: 1.1596e-05\n",
            "Epoch 10: val_loss did not improve from 0.00000\n",
            "4236/4236 [==============================] - 8s 2ms/step - loss: 1.1588e-05 - val_loss: 1.1350e-07\n",
            "17649/17649 [==============================] - 21s 1ms/step\n",
            "17649/17649 [==============================] - 22s 1ms/step\n",
            "2644/2644 [==============================] - 3s 1ms/step\n",
            "reached auc of 0.5912289096445648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/Folder_To_Zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtvG0tZBSYts",
        "outputId": "c847e689-6825-4900-8073-b85674381931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/Folder_To_Zip\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/file.zip . -i /content/Folder_To_Zip)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/results.zip /content/results"
      ],
      "metadata": {
        "id": "aS7YD6TnVvE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee12bab-8d41-459e-b771-4a4965b4d268"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/results/ (stored 0%)\n",
            "  adding: content/results/30/ (stored 0%)\n",
            "  adding: content/results/30/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/30/result.npz (deflated 1%)\n",
            "  adding: content/results/30/model.tf.index (deflated 54%)\n",
            "  adding: content/results/30/checkpoint (deflated 40%)\n",
            "  adding: content/results/51/ (stored 0%)\n",
            "  adding: content/results/51/model.tf.data-00000-of-00001 (deflated 41%)\n",
            "  adding: content/results/51/result.npz (deflated 1%)\n",
            "  adding: content/results/51/model.tf.index (deflated 54%)\n",
            "  adding: content/results/51/checkpoint (deflated 40%)\n",
            "  adding: content/results/67/ (stored 0%)\n",
            "  adding: content/results/67/model.tf.data-00000-of-00001 (deflated 42%)\n",
            "  adding: content/results/67/result.npz (deflated 1%)\n",
            "  adding: content/results/67/model.tf.index (deflated 54%)\n",
            "  adding: content/results/67/checkpoint (deflated 40%)\n",
            "  adding: content/results/34/ (stored 0%)\n",
            "  adding: content/results/34/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/34/result.npz (deflated 1%)\n",
            "  adding: content/results/34/model.tf.index (deflated 54%)\n",
            "  adding: content/results/34/checkpoint (deflated 40%)\n",
            "  adding: content/results/1/ (stored 0%)\n",
            "  adding: content/results/1/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/1/result.npz (deflated 1%)\n",
            "  adding: content/results/1/model.tf.index (deflated 54%)\n",
            "  adding: content/results/1/checkpoint (deflated 40%)\n",
            "  adding: content/results/33/ (stored 0%)\n",
            "  adding: content/results/33/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/33/result.npz (deflated 1%)\n",
            "  adding: content/results/33/model.tf.index (deflated 54%)\n",
            "  adding: content/results/33/checkpoint (deflated 40%)\n",
            "  adding: content/results/5/ (stored 0%)\n",
            "  adding: content/results/5/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/5/result.npz (deflated 0%)\n",
            "  adding: content/results/5/model.tf.index (deflated 54%)\n",
            "  adding: content/results/5/checkpoint (deflated 40%)\n",
            "  adding: content/results/79/ (stored 0%)\n",
            "  adding: content/results/79/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/79/result.npz (deflated 1%)\n",
            "  adding: content/results/79/model.tf.index (deflated 54%)\n",
            "  adding: content/results/79/checkpoint (deflated 40%)\n",
            "  adding: content/results/71/ (stored 0%)\n",
            "  adding: content/results/71/model.tf.data-00000-of-00001 (deflated 50%)\n",
            "  adding: content/results/71/result.npz (deflated 0%)\n",
            "  adding: content/results/71/model.tf.index (deflated 54%)\n",
            "  adding: content/results/71/checkpoint (deflated 40%)\n",
            "  adding: content/results/91/ (stored 0%)\n",
            "  adding: content/results/91/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/91/result.npz (deflated 0%)\n",
            "  adding: content/results/91/model.tf.index (deflated 54%)\n",
            "  adding: content/results/91/checkpoint (deflated 40%)\n",
            "  adding: content/results/88/ (stored 0%)\n",
            "  adding: content/results/88/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/88/result.npz (deflated 1%)\n",
            "  adding: content/results/88/model.tf.index (deflated 54%)\n",
            "  adding: content/results/88/checkpoint (deflated 40%)\n",
            "  adding: content/results/48/ (stored 0%)\n",
            "  adding: content/results/48/model.tf.data-00000-of-00001 (deflated 55%)\n",
            "  adding: content/results/48/result.npz (deflated 1%)\n",
            "  adding: content/results/48/model.tf.index (deflated 54%)\n",
            "  adding: content/results/48/checkpoint (deflated 40%)\n",
            "  adding: content/results/6/ (stored 0%)\n",
            "  adding: content/results/6/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/6/result.npz (deflated 0%)\n",
            "  adding: content/results/6/model.tf.index (deflated 54%)\n",
            "  adding: content/results/6/checkpoint (deflated 40%)\n",
            "  adding: content/results/90/ (stored 0%)\n",
            "  adding: content/results/90/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/90/result.npz (deflated 1%)\n",
            "  adding: content/results/90/model.tf.index (deflated 54%)\n",
            "  adding: content/results/90/checkpoint (deflated 40%)\n",
            "  adding: content/results/28/ (stored 0%)\n",
            "  adding: content/results/28/model.tf.data-00000-of-00001 (deflated 54%)\n",
            "  adding: content/results/28/result.npz (deflated 1%)\n",
            "  adding: content/results/28/model.tf.index (deflated 54%)\n",
            "  adding: content/results/28/checkpoint (deflated 40%)\n",
            "  adding: content/results/35/ (stored 0%)\n",
            "  adding: content/results/35/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/35/result.npz (deflated 1%)\n",
            "  adding: content/results/35/model.tf.index (deflated 54%)\n",
            "  adding: content/results/35/checkpoint (deflated 40%)\n",
            "  adding: content/results/93/ (stored 0%)\n",
            "  adding: content/results/93/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/93/result.npz (deflated 0%)\n",
            "  adding: content/results/93/model.tf.index (deflated 54%)\n",
            "  adding: content/results/93/checkpoint (deflated 40%)\n",
            "  adding: content/results/82/ (stored 0%)\n",
            "  adding: content/results/82/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/82/result.npz (deflated 1%)\n",
            "  adding: content/results/82/model.tf.index (deflated 54%)\n",
            "  adding: content/results/82/checkpoint (deflated 40%)\n",
            "  adding: content/results/63/ (stored 0%)\n",
            "  adding: content/results/63/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/63/result.npz (deflated 0%)\n",
            "  adding: content/results/63/model.tf.index (deflated 54%)\n",
            "  adding: content/results/63/checkpoint (deflated 40%)\n",
            "  adding: content/results/47/ (stored 0%)\n",
            "  adding: content/results/47/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/47/result.npz (deflated 1%)\n",
            "  adding: content/results/47/model.tf.index (deflated 54%)\n",
            "  adding: content/results/47/checkpoint (deflated 40%)\n",
            "  adding: content/results/84/ (stored 0%)\n",
            "  adding: content/results/84/model.tf.data-00000-of-00001 (deflated 52%)\n",
            "  adding: content/results/84/result.npz (deflated 1%)\n",
            "  adding: content/results/84/model.tf.index (deflated 54%)\n",
            "  adding: content/results/84/checkpoint (deflated 40%)\n",
            "  adding: content/results/73/ (stored 0%)\n",
            "  adding: content/results/73/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/73/result.npz (deflated 1%)\n",
            "  adding: content/results/73/model.tf.index (deflated 54%)\n",
            "  adding: content/results/73/checkpoint (deflated 40%)\n",
            "  adding: content/results/12/ (stored 0%)\n",
            "  adding: content/results/12/model.tf.data-00000-of-00001 (deflated 50%)\n",
            "  adding: content/results/12/result.npz (deflated 0%)\n",
            "  adding: content/results/12/model.tf.index (deflated 54%)\n",
            "  adding: content/results/12/checkpoint (deflated 40%)\n",
            "  adding: content/results/61/ (stored 0%)\n",
            "  adding: content/results/61/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/61/result.npz (deflated 0%)\n",
            "  adding: content/results/61/model.tf.index (deflated 54%)\n",
            "  adding: content/results/61/checkpoint (deflated 40%)\n",
            "  adding: content/results/31/ (stored 0%)\n",
            "  adding: content/results/31/model.tf.data-00000-of-00001 (deflated 50%)\n",
            "  adding: content/results/31/result.npz (deflated 1%)\n",
            "  adding: content/results/31/model.tf.index (deflated 54%)\n",
            "  adding: content/results/31/checkpoint (deflated 40%)\n",
            "  adding: content/results/40/ (stored 0%)\n",
            "  adding: content/results/40/model.tf.data-00000-of-00001 (deflated 50%)\n",
            "  adding: content/results/40/result.npz (deflated 3%)\n",
            "  adding: content/results/40/model.tf.index (deflated 54%)\n",
            "  adding: content/results/40/checkpoint (deflated 40%)\n",
            "  adding: content/results/77/ (stored 0%)\n",
            "  adding: content/results/77/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/77/result.npz (deflated 1%)\n",
            "  adding: content/results/77/model.tf.index (deflated 54%)\n",
            "  adding: content/results/77/checkpoint (deflated 40%)\n",
            "  adding: content/results/44/ (stored 0%)\n",
            "  adding: content/results/44/model.tf.data-00000-of-00001 (deflated 40%)\n",
            "  adding: content/results/44/result.npz (deflated 1%)\n",
            "  adding: content/results/44/model.tf.index (deflated 54%)\n",
            "  adding: content/results/44/checkpoint (deflated 40%)\n",
            "  adding: content/results/57/ (stored 0%)\n",
            "  adding: content/results/57/model.tf.data-00000-of-00001 (deflated 53%)\n",
            "  adding: content/results/57/result.npz (deflated 2%)\n",
            "  adding: content/results/57/model.tf.index (deflated 54%)\n",
            "  adding: content/results/57/checkpoint (deflated 40%)\n",
            "  adding: content/results/19/ (stored 0%)\n",
            "  adding: content/results/19/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/19/result.npz (deflated 0%)\n",
            "  adding: content/results/19/model.tf.index (deflated 54%)\n",
            "  adding: content/results/19/checkpoint (deflated 40%)\n",
            "  adding: content/results/2/ (stored 0%)\n",
            "  adding: content/results/2/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/2/result.npz (deflated 1%)\n",
            "  adding: content/results/2/model.tf.index (deflated 54%)\n",
            "  adding: content/results/2/checkpoint (deflated 40%)\n",
            "  adding: content/results/75/ (stored 0%)\n",
            "  adding: content/results/75/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/75/result.npz (deflated 1%)\n",
            "  adding: content/results/75/model.tf.index (deflated 54%)\n",
            "  adding: content/results/75/checkpoint (deflated 40%)\n",
            "  adding: content/results/22/ (stored 0%)\n",
            "  adding: content/results/22/model.tf.data-00000-of-00001 (deflated 53%)\n",
            "  adding: content/results/22/result.npz (deflated 1%)\n",
            "  adding: content/results/22/model.tf.index (deflated 54%)\n",
            "  adding: content/results/22/checkpoint (deflated 40%)\n",
            "  adding: content/results/69/ (stored 0%)\n",
            "  adding: content/results/69/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/69/result.npz (deflated 1%)\n",
            "  adding: content/results/69/model.tf.index (deflated 54%)\n",
            "  adding: content/results/69/checkpoint (deflated 40%)\n",
            "  adding: content/results/68/ (stored 0%)\n",
            "  adding: content/results/68/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/68/result.npz (deflated 1%)\n",
            "  adding: content/results/68/model.tf.index (deflated 54%)\n",
            "  adding: content/results/68/checkpoint (deflated 40%)\n",
            "  adding: content/results/21/ (stored 0%)\n",
            "  adding: content/results/21/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/21/result.npz (deflated 0%)\n",
            "  adding: content/results/21/model.tf.index (deflated 54%)\n",
            "  adding: content/results/21/checkpoint (deflated 40%)\n",
            "  adding: content/results/97/ (stored 0%)\n",
            "  adding: content/results/97/model.tf.data-00000-of-00001 (deflated 53%)\n",
            "  adding: content/results/97/result.npz (deflated 1%)\n",
            "  adding: content/results/97/model.tf.index (deflated 54%)\n",
            "  adding: content/results/97/checkpoint (deflated 40%)\n",
            "  adding: content/results/56/ (stored 0%)\n",
            "  adding: content/results/56/model.tf.data-00000-of-00001 (deflated 52%)\n",
            "  adding: content/results/56/result.npz (deflated 1%)\n",
            "  adding: content/results/56/model.tf.index (deflated 54%)\n",
            "  adding: content/results/56/checkpoint (deflated 40%)\n",
            "  adding: content/results/3/ (stored 0%)\n",
            "  adding: content/results/3/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/3/result.npz (deflated 2%)\n",
            "  adding: content/results/3/model.tf.index (deflated 54%)\n",
            "  adding: content/results/3/checkpoint (deflated 40%)\n",
            "  adding: content/results/74/ (stored 0%)\n",
            "  adding: content/results/74/model.tf.data-00000-of-00001 (deflated 52%)\n",
            "  adding: content/results/74/result.npz (deflated 6%)\n",
            "  adding: content/results/74/model.tf.index (deflated 54%)\n",
            "  adding: content/results/74/checkpoint (deflated 40%)\n",
            "  adding: content/results/55/ (stored 0%)\n",
            "  adding: content/results/55/model.tf.data-00000-of-00001 (deflated 41%)\n",
            "  adding: content/results/55/result.npz (deflated 1%)\n",
            "  adding: content/results/55/model.tf.index (deflated 54%)\n",
            "  adding: content/results/55/checkpoint (deflated 40%)\n",
            "  adding: content/results/0/ (stored 0%)\n",
            "  adding: content/results/0/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/0/result.npz (deflated 1%)\n",
            "  adding: content/results/0/model.tf.index (deflated 54%)\n",
            "  adding: content/results/0/checkpoint (deflated 40%)\n",
            "  adding: content/results/36/ (stored 0%)\n",
            "  adding: content/results/36/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/36/result.npz (deflated 1%)\n",
            "  adding: content/results/36/model.tf.index (deflated 54%)\n",
            "  adding: content/results/36/checkpoint (deflated 40%)\n",
            "  adding: content/results/27/ (stored 0%)\n",
            "  adding: content/results/27/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/27/result.npz (deflated 1%)\n",
            "  adding: content/results/27/model.tf.index (deflated 54%)\n",
            "  adding: content/results/27/checkpoint (deflated 40%)\n",
            "  adding: content/results/24/ (stored 0%)\n",
            "  adding: content/results/24/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/24/result.npz (deflated 1%)\n",
            "  adding: content/results/24/model.tf.index (deflated 54%)\n",
            "  adding: content/results/24/checkpoint (deflated 40%)\n",
            "  adding: content/results/58/ (stored 0%)\n",
            "  adding: content/results/58/model.tf.data-00000-of-00001 (deflated 53%)\n",
            "  adding: content/results/58/result.npz (deflated 3%)\n",
            "  adding: content/results/58/model.tf.index (deflated 54%)\n",
            "  adding: content/results/58/checkpoint (deflated 40%)\n",
            "  adding: content/results/45/ (stored 0%)\n",
            "  adding: content/results/45/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/45/result.npz (deflated 1%)\n",
            "  adding: content/results/45/model.tf.index (deflated 54%)\n",
            "  adding: content/results/45/checkpoint (deflated 40%)\n",
            "  adding: content/results/96/ (stored 0%)\n",
            "  adding: content/results/96/model.tf.data-00000-of-00001 (deflated 55%)\n",
            "  adding: content/results/96/result.npz (deflated 2%)\n",
            "  adding: content/results/96/model.tf.index (deflated 54%)\n",
            "  adding: content/results/96/checkpoint (deflated 40%)\n",
            "  adding: content/results/50/ (stored 0%)\n",
            "  adding: content/results/50/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/50/result.npz (deflated 1%)\n",
            "  adding: content/results/50/model.tf.index (deflated 54%)\n",
            "  adding: content/results/50/checkpoint (deflated 40%)\n",
            "  adding: content/results/89/ (stored 0%)\n",
            "  adding: content/results/89/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/89/result.npz (deflated 0%)\n",
            "  adding: content/results/89/model.tf.index (deflated 54%)\n",
            "  adding: content/results/89/checkpoint (deflated 40%)\n",
            "  adding: content/results/42/ (stored 0%)\n",
            "  adding: content/results/42/model.tf.data-00000-of-00001 (deflated 39%)\n",
            "  adding: content/results/42/result.npz (deflated 0%)\n",
            "  adding: content/results/42/model.tf.index (deflated 54%)\n",
            "  adding: content/results/42/checkpoint (deflated 40%)\n",
            "  adding: content/results/98/ (stored 0%)\n",
            "  adding: content/results/98/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/98/result.npz (deflated 1%)\n",
            "  adding: content/results/98/model.tf.index (deflated 54%)\n",
            "  adding: content/results/98/checkpoint (deflated 40%)\n",
            "  adding: content/results/65/ (stored 0%)\n",
            "  adding: content/results/65/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/65/result.npz (deflated 1%)\n",
            "  adding: content/results/65/model.tf.index (deflated 54%)\n",
            "  adding: content/results/65/checkpoint (deflated 40%)\n",
            "  adding: content/results/52/ (stored 0%)\n",
            "  adding: content/results/52/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/52/result.npz (deflated 0%)\n",
            "  adding: content/results/52/model.tf.index (deflated 54%)\n",
            "  adding: content/results/52/checkpoint (deflated 40%)\n",
            "  adding: content/results/13/ (stored 0%)\n",
            "  adding: content/results/13/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/13/result.npz (deflated 1%)\n",
            "  adding: content/results/13/model.tf.index (deflated 54%)\n",
            "  adding: content/results/13/checkpoint (deflated 40%)\n",
            "  adding: content/results/16/ (stored 0%)\n",
            "  adding: content/results/16/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/16/result.npz (deflated 1%)\n",
            "  adding: content/results/16/model.tf.index (deflated 54%)\n",
            "  adding: content/results/16/checkpoint (deflated 40%)\n",
            "  adding: content/results/26/ (stored 0%)\n",
            "  adding: content/results/26/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/26/result.npz (deflated 1%)\n",
            "  adding: content/results/26/model.tf.index (deflated 54%)\n",
            "  adding: content/results/26/checkpoint (deflated 40%)\n",
            "  adding: content/results/59/ (stored 0%)\n",
            "  adding: content/results/59/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/59/result.npz (deflated 1%)\n",
            "  adding: content/results/59/model.tf.index (deflated 54%)\n",
            "  adding: content/results/59/checkpoint (deflated 40%)\n",
            "  adding: content/results/64/ (stored 0%)\n",
            "  adding: content/results/64/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/64/result.npz (deflated 3%)\n",
            "  adding: content/results/64/model.tf.index (deflated 54%)\n",
            "  adding: content/results/64/checkpoint (deflated 40%)\n",
            "  adding: content/results/14/ (stored 0%)\n",
            "  adding: content/results/14/model.tf.data-00000-of-00001 (deflated 54%)\n",
            "  adding: content/results/14/result.npz (deflated 2%)\n",
            "  adding: content/results/14/model.tf.index (deflated 54%)\n",
            "  adding: content/results/14/checkpoint (deflated 40%)\n",
            "  adding: content/results/39/ (stored 0%)\n",
            "  adding: content/results/39/model.tf.data-00000-of-00001 (deflated 43%)\n",
            "  adding: content/results/39/result.npz (deflated 1%)\n",
            "  adding: content/results/39/model.tf.index (deflated 54%)\n",
            "  adding: content/results/39/checkpoint (deflated 40%)\n",
            "  adding: content/results/81/ (stored 0%)\n",
            "  adding: content/results/81/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/81/result.npz (deflated 0%)\n",
            "  adding: content/results/81/model.tf.index (deflated 54%)\n",
            "  adding: content/results/81/checkpoint (deflated 40%)\n",
            "  adding: content/results/23/ (stored 0%)\n",
            "  adding: content/results/23/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/23/result.npz (deflated 0%)\n",
            "  adding: content/results/23/model.tf.index (deflated 54%)\n",
            "  adding: content/results/23/checkpoint (deflated 40%)\n",
            "  adding: content/results/99/ (stored 0%)\n",
            "  adding: content/results/99/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/99/result.npz (deflated 1%)\n",
            "  adding: content/results/99/model.tf.index (deflated 54%)\n",
            "  adding: content/results/99/checkpoint (deflated 40%)\n",
            "  adding: content/results/62/ (stored 0%)\n",
            "  adding: content/results/62/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/62/result.npz (deflated 1%)\n",
            "  adding: content/results/62/model.tf.index (deflated 54%)\n",
            "  adding: content/results/62/checkpoint (deflated 40%)\n",
            "  adding: content/results/20/ (stored 0%)\n",
            "  adding: content/results/20/model.tf.data-00000-of-00001 (deflated 40%)\n",
            "  adding: content/results/20/result.npz (deflated 0%)\n",
            "  adding: content/results/20/model.tf.index (deflated 54%)\n",
            "  adding: content/results/20/checkpoint (deflated 40%)\n",
            "  adding: content/results/29/ (stored 0%)\n",
            "  adding: content/results/29/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/29/result.npz (deflated 1%)\n",
            "  adding: content/results/29/model.tf.index (deflated 54%)\n",
            "  adding: content/results/29/checkpoint (deflated 40%)\n",
            "  adding: content/results/18/ (stored 0%)\n",
            "  adding: content/results/18/model.tf.data-00000-of-00001 (deflated 41%)\n",
            "  adding: content/results/18/result.npz (deflated 1%)\n",
            "  adding: content/results/18/model.tf.index (deflated 54%)\n",
            "  adding: content/results/18/checkpoint (deflated 40%)\n",
            "  adding: content/results/60/ (stored 0%)\n",
            "  adding: content/results/60/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/60/result.npz (deflated 1%)\n",
            "  adding: content/results/60/model.tf.index (deflated 54%)\n",
            "  adding: content/results/60/checkpoint (deflated 40%)\n",
            "  adding: content/results/54/ (stored 0%)\n",
            "  adding: content/results/54/model.tf.data-00000-of-00001 (deflated 40%)\n",
            "  adding: content/results/54/result.npz (deflated 1%)\n",
            "  adding: content/results/54/model.tf.index (deflated 54%)\n",
            "  adding: content/results/54/checkpoint (deflated 40%)\n",
            "  adding: content/results/95/ (stored 0%)\n",
            "  adding: content/results/95/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/95/result.npz (deflated 0%)\n",
            "  adding: content/results/95/model.tf.index (deflated 54%)\n",
            "  adding: content/results/95/checkpoint (deflated 40%)\n",
            "  adding: content/results/8/ (stored 0%)\n",
            "  adding: content/results/8/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/8/result.npz (deflated 0%)\n",
            "  adding: content/results/8/model.tf.index (deflated 54%)\n",
            "  adding: content/results/8/checkpoint (deflated 40%)\n",
            "  adding: content/results/7/ (stored 0%)\n",
            "  adding: content/results/7/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/7/result.npz (deflated 1%)\n",
            "  adding: content/results/7/model.tf.index (deflated 54%)\n",
            "  adding: content/results/7/checkpoint (deflated 40%)\n",
            "  adding: content/results/10/ (stored 0%)\n",
            "  adding: content/results/10/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/10/result.npz (deflated 1%)\n",
            "  adding: content/results/10/model.tf.index (deflated 54%)\n",
            "  adding: content/results/10/checkpoint (deflated 40%)\n",
            "  adding: content/results/15/ (stored 0%)\n",
            "  adding: content/results/15/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/15/result.npz (deflated 1%)\n",
            "  adding: content/results/15/model.tf.index (deflated 54%)\n",
            "  adding: content/results/15/checkpoint (deflated 40%)\n",
            "  adding: content/results/94/ (stored 0%)\n",
            "  adding: content/results/94/model.tf.data-00000-of-00001 (deflated 42%)\n",
            "  adding: content/results/94/result.npz (deflated 1%)\n",
            "  adding: content/results/94/model.tf.index (deflated 54%)\n",
            "  adding: content/results/94/checkpoint (deflated 40%)\n",
            "  adding: content/results/72/ (stored 0%)\n",
            "  adding: content/results/72/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/72/result.npz (deflated 1%)\n",
            "  adding: content/results/72/model.tf.index (deflated 54%)\n",
            "  adding: content/results/72/checkpoint (deflated 40%)\n",
            "  adding: content/results/38/ (stored 0%)\n",
            "  adding: content/results/38/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/38/result.npz (deflated 1%)\n",
            "  adding: content/results/38/model.tf.index (deflated 54%)\n",
            "  adding: content/results/38/checkpoint (deflated 40%)\n",
            "  adding: content/results/32/ (stored 0%)\n",
            "  adding: content/results/32/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/32/result.npz (deflated 1%)\n",
            "  adding: content/results/32/model.tf.index (deflated 54%)\n",
            "  adding: content/results/32/checkpoint (deflated 40%)\n",
            "  adding: content/results/4/ (stored 0%)\n",
            "  adding: content/results/4/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/4/result.npz (deflated 1%)\n",
            "  adding: content/results/4/model.tf.index (deflated 54%)\n",
            "  adding: content/results/4/checkpoint (deflated 40%)\n",
            "  adding: content/results/43/ (stored 0%)\n",
            "  adding: content/results/43/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/43/result.npz (deflated 1%)\n",
            "  adding: content/results/43/model.tf.index (deflated 54%)\n",
            "  adding: content/results/43/checkpoint (deflated 40%)\n",
            "  adding: content/results/76/ (stored 0%)\n",
            "  adding: content/results/76/model.tf.data-00000-of-00001 (deflated 53%)\n",
            "  adding: content/results/76/result.npz (deflated 1%)\n",
            "  adding: content/results/76/model.tf.index (deflated 54%)\n",
            "  adding: content/results/76/checkpoint (deflated 40%)\n",
            "  adding: content/results/87/ (stored 0%)\n",
            "  adding: content/results/87/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/87/result.npz (deflated 0%)\n",
            "  adding: content/results/87/model.tf.index (deflated 54%)\n",
            "  adding: content/results/87/checkpoint (deflated 40%)\n",
            "  adding: content/results/66/ (stored 0%)\n",
            "  adding: content/results/66/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/66/result.npz (deflated 1%)\n",
            "  adding: content/results/66/model.tf.index (deflated 54%)\n",
            "  adding: content/results/66/checkpoint (deflated 40%)\n",
            "  adding: content/results/9/ (stored 0%)\n",
            "  adding: content/results/9/model.tf.data-00000-of-00001 (deflated 52%)\n",
            "  adding: content/results/9/result.npz (deflated 1%)\n",
            "  adding: content/results/9/model.tf.index (deflated 54%)\n",
            "  adding: content/results/9/checkpoint (deflated 40%)\n",
            "  adding: content/results/41/ (stored 0%)\n",
            "  adding: content/results/41/model.tf.data-00000-of-00001 (deflated 51%)\n",
            "  adding: content/results/41/result.npz (deflated 1%)\n",
            "  adding: content/results/41/model.tf.index (deflated 54%)\n",
            "  adding: content/results/41/checkpoint (deflated 40%)\n",
            "  adding: content/results/46/ (stored 0%)\n",
            "  adding: content/results/46/model.tf.data-00000-of-00001 (deflated 52%)\n",
            "  adding: content/results/46/result.npz (deflated 1%)\n",
            "  adding: content/results/46/model.tf.index (deflated 54%)\n",
            "  adding: content/results/46/checkpoint (deflated 40%)\n",
            "  adding: content/results/11/ (stored 0%)\n",
            "  adding: content/results/11/model.tf.data-00000-of-00001 (deflated 44%)\n",
            "  adding: content/results/11/result.npz (deflated 0%)\n",
            "  adding: content/results/11/model.tf.index (deflated 54%)\n",
            "  adding: content/results/11/checkpoint (deflated 40%)\n",
            "  adding: content/results/83/ (stored 0%)\n",
            "  adding: content/results/83/model.tf.data-00000-of-00001 (deflated 48%)\n",
            "  adding: content/results/83/result.npz (deflated 1%)\n",
            "  adding: content/results/83/model.tf.index (deflated 54%)\n",
            "  adding: content/results/83/checkpoint (deflated 40%)\n",
            "  adding: content/results/17/ (stored 0%)\n",
            "  adding: content/results/17/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/17/result.npz (deflated 1%)\n",
            "  adding: content/results/17/model.tf.index (deflated 54%)\n",
            "  adding: content/results/17/checkpoint (deflated 40%)\n",
            "  adding: content/results/53/ (stored 0%)\n",
            "  adding: content/results/53/model.tf.data-00000-of-00001 (deflated 39%)\n",
            "  adding: content/results/53/result.npz (deflated 0%)\n",
            "  adding: content/results/53/model.tf.index (deflated 54%)\n",
            "  adding: content/results/53/checkpoint (deflated 40%)\n",
            "  adding: content/results/25/ (stored 0%)\n",
            "  adding: content/results/25/model.tf.data-00000-of-00001 (deflated 49%)\n",
            "  adding: content/results/25/result.npz (deflated 0%)\n",
            "  adding: content/results/25/model.tf.index (deflated 54%)\n",
            "  adding: content/results/25/checkpoint (deflated 40%)\n",
            "  adding: content/results/86/ (stored 0%)\n",
            "  adding: content/results/86/model.tf.data-00000-of-00001 (deflated 46%)\n",
            "  adding: content/results/86/result.npz (deflated 1%)\n",
            "  adding: content/results/86/model.tf.index (deflated 54%)\n",
            "  adding: content/results/86/checkpoint (deflated 40%)\n",
            "  adding: content/results/80/ (stored 0%)\n",
            "  adding: content/results/80/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/80/result.npz (deflated 1%)\n",
            "  adding: content/results/80/model.tf.index (deflated 54%)\n",
            "  adding: content/results/80/checkpoint (deflated 40%)\n",
            "  adding: content/results/37/ (stored 0%)\n",
            "  adding: content/results/37/model.tf.data-00000-of-00001 (deflated 41%)\n",
            "  adding: content/results/37/result.npz (deflated 1%)\n",
            "  adding: content/results/37/model.tf.index (deflated 54%)\n",
            "  adding: content/results/37/checkpoint (deflated 40%)\n",
            "  adding: content/results/49/ (stored 0%)\n",
            "  adding: content/results/49/model.tf.data-00000-of-00001 (deflated 50%)\n",
            "  adding: content/results/49/result.npz (deflated 1%)\n",
            "  adding: content/results/49/model.tf.index (deflated 54%)\n",
            "  adding: content/results/49/checkpoint (deflated 40%)\n",
            "  adding: content/results/78/ (stored 0%)\n",
            "  adding: content/results/78/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/78/result.npz (deflated 1%)\n",
            "  adding: content/results/78/model.tf.index (deflated 54%)\n",
            "  adding: content/results/78/checkpoint (deflated 40%)\n",
            "  adding: content/results/92/ (stored 0%)\n",
            "  adding: content/results/92/model.tf.data-00000-of-00001 (deflated 47%)\n",
            "  adding: content/results/92/result.npz (deflated 1%)\n",
            "  adding: content/results/92/model.tf.index (deflated 54%)\n",
            "  adding: content/results/92/checkpoint (deflated 40%)\n",
            "  adding: content/results/70/ (stored 0%)\n",
            "  adding: content/results/70/model.tf.data-00000-of-00001 (deflated 45%)\n",
            "  adding: content/results/70/result.npz (deflated 1%)\n",
            "  adding: content/results/70/model.tf.index (deflated 54%)\n",
            "  adding: content/results/70/checkpoint (deflated 40%)\n",
            "  adding: content/results/85/ (stored 0%)\n",
            "  adding: content/results/85/model.tf.data-00000-of-00001 (deflated 40%)\n",
            "  adding: content/results/85/result.npz (deflated 1%)\n",
            "  adding: content/results/85/model.tf.index (deflated 54%)\n",
            "  adding: content/results/85/checkpoint (deflated 40%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnFmwOKpPs8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}